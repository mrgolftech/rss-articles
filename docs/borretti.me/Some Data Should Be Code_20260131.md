# Some Data Should Be Code

**来源:** [borretti.me](https://borretti.me/feed)
**发布时间:** Sat, 31 Jan 2026 00:00:00 +0000
**链接:** https://borretti.me/article/some-data-should-be-code

---

{'type': 'text/html', 'language': None, 'base': '', 'value': '
I write a lot of
Makefiles
. I use it not as a command runner but as an ad-hoc build system for small projects, typically for compiling Markdown documents and their dependencies. Like so:
\n\n
\n\n
And the above graph was generated by this very simple Makefile:
\n\n
\n
\n
graph.png
:
graph.dot
\n
dot
-Tpng
$<
-o
$@
\n\n
clean
:
\n
rm
-f
graph.png
\n
\n
\n
\n\n
(I could never remember the
automatic variable
syntax until I made
flashcards
for them.)
\n\n
It works for simple projects, when you can mostly hand-write the rules. But the abstraction ceiling is very low. If you have a bunch of almost identical rules, e.g.:
\n\n
\n
\n
a.png
:
a.csv plot.py
\n
python
plot.py
$<
$@
\n\n
b.png
:
b.csv plot.py
\n
python
plot.py
$<
$@
\n\n
c.png
:
c.csv plot.py
\n
python
plot.py
$<
$@
\n
\n
\n
\n\n
You can use pattern-matching to them into a “rule schema”, by analogy to axiom schemata:
\n\n
\n
\n
%.png
:
%.csv plot.py
\n
python
plot.py
$<
$@
\n
\n
\n
\n\n
Which works backwards: when something in the build graph depends on a target matching
%.png
, Make synthesizes a rule instance with a dependency on the corresponding
.csv
file.
\n\n
But pattern matching is still very limited. Lately I’ve been building my own
plain-text accounting
solution using some Python scripts. One of the tasks is to read a CSV of bank transactions from 2019–2024 and split it into TOML files for each year-month, to make subsequent processing parallelizable. So the rules might be something like:
\n\n
ledger/2019-08.toml: inputs/checkbook_pro_export.csv\n    uv run import_from_checkbook.py --year=2019 --month=8\n\nledger/2019-09.toml: inputs/checkbook_pro_export.csv\n    uv run import_from_checkbook.py --year=2019 --month=9\n\n# ...\n
\n\n
I had to write a Python script to generate the complete Makefile. Makefiles look like code, but are data: they are a container format for tiny fragments of shell that are run on-demand by the Make engine. And because Make doesn’t scale, for complex tasks you have to bring out a real programming language to generate the Makefile.
\n\n
I wish I could, instead, write a
make.py
file with something like this:
\n\n
\n
\n
from
whatever
import
*
\n\n
g
=
BuildGraph
()
\n\n
EXPORT
:
str
=
"inputs/checkbook_pro_export.csv"
\n\n
# The (year, month) pairs I have bank transaction CSVs for.\n
year_months
:
list
[
tuple
[
int
,
int
]]
=
[
\n
(
y
,
m
)
for
y
in
range
(
2019
,
2026
)
for
m
in
range
(
1
,
13
)
\n
]
\n\n
# Import transactions for each year-month into a separate ledger.\n
for
year
,
month
in
year_months
:
\n
ledger_path
:
str
=
f
"ledger/
{
year
}
_
{
month
:
02
d
}
.toml"
\n
g
.
rule
(
\n
targets
=
[
ledger_path
],
\n
deps
=
[
EXPORT
],
\n
fn
=
lambda
:
import_from_checkbook
(
ledger_path
,
year
,
month
),
\n
)
\n
\n
\n
\n\n
Fortunately this exists: it’s called
doit
, but it’s not widely known.
\n\n
\n\n
A lot of things are like Makefiles: data that should be lifted one level up to become code.
\n\n
Consider
CloudFormation
. Nobody likes writing those massive YAML files by hand, so AWS introduced
CDK
, which is literally just a library
1
of classes that represent AWS resources. Running a CDK program emits CloudFormation YAML as though it were an assembly language for infrastructure. And so you get type safety, modularity, abstraction, conditionals and loops, all for free.
\n\n
Consider
GitHub Actions
. How much better off would we be if, instead of writing the workflow-job-step tree by hand, we could just have a single Python script, executed on push, whose output is the GitHub Actions YAML-as-assembly? So you might write:
\n\n
\n
\n
from
ga
import
*
\n
from
checkout_action
import
CheckoutAction
\n
from
rust_action
import
RustSetupAction
\n\n
# Define the workflow that runs on each commit.\n
commit_workflow
=
Workflow
(
\n
name
=
"commit"
,
\n
test
=
lambda
ev
:
isinstance
(
ev
,
CommitEvent
),
\n
jobs
=
[
\n
# The lint job.\n
Job
(
\n
name
=
"lint"
,
\n
steps
=
[
\n
Step
(
\n
name
=
"check out"
,
\n
run
=
CheckoutAction
(),
\n
),
\n
Step
(
\n
name
=
"set up Rust and Cargo"
,
\n
run
=
RustSetupAction
(),
\n
),
\n
Step
(
\n
name
=
"run cargo fmt"
,
\n
run
=
Shell
([
"cargo"
,
"fmt"
,
"--check"
])
\n
)
\n
]
\n
)
\n
]
\n
)
\n
\n
\n
\n\n
Actions here would simply be ordinary Python libraries the CI script depends on. Again: conditions, loops, abstraction, type safety, we get all of those for free by virtue of using a language that was designed to be a language, rather than a data exchange language that slowly grows into a poorly-designed DSL.
\n\n
Why do we repeatedly end up here? Static data has better safety/static analysis properties than code, but I don’t think that’s foremost in mind when people design these systems. Besides, using code to emit data (as CDK does) gives you those exact same properties. Rather, I think some people think it’s cute and clever to build tiny DSLs in a data format. They’re proud that they can get away with a “simple”, static solution rather than a dynamic one.
\n\n
If you’re building a new CI system/IaC platform/Make replacement: please just let me write code to dynamically create the workflow/infrastructure/build graph.
\n\n
Footnotes
\n\n
\n
\n
\n\n
Or rather, a polyglot collection of libraries, one per language, like
Pulumi
.\xa0
↩
\n
\n
\n
'}

---

*抓取时间: 2026-02-05 12:56:49*
