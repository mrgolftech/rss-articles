# Notes on Lagrange Interpolating Polynomials

**来源:** [eli.thegreenplace.net](https://eli.thegreenplace.net)
**发布时间:** 2026-02-28T18:58:00-08:00
**链接:** https://eli.thegreenplace.net/2026/notes-on-lagrange-interpolating-polynomials/

---

{'type': 'text/html', 'language': None, 'base': 'https://eli.thegreenplace.net/feeds/all.atom.xml', 'value': '<p><em>Polynomial interpolation</em> is a method of finding a polynomial function\nthat fits a given set of data perfectly. More concretely, suppose we\nhave a set of n+1 distinct points <a class="footnote-reference" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-1" id="footnote-reference-1">[1]</a>:</p>\n\\[(x_0,y_0), (x_1, y_1), (x_2, y_2)\\cdots(x_n, y_n)\\]\n<p>And we want to find the polynomial coefficients {a_0\\cdots a_n}\nsuch that:</p>\n\\[p(x)=a_0 + a_1 x + a_2 x^2 + \\cdots + a_n x^n\\]\n<p>Fits all our points; that is p(x_0)=y_0, p(x_1)=y_1 etc.</p>\n<p>This post discusses a common approach to solving this problem, and also\nshows why such a polynomial exists and is unique.</p>\n<div class="section" id="showing-existence-using-linear-algebra">\n<h2>Showing existence using linear algebra</h2>\n<p>When we assign all points (x_i, y_i) into the generic polynomial\np(x), we get:</p>\n\\[\\begin{aligned}\np(x_0)&amp;=a_0 + a_1 x_0 + a_2 x_0^2 + \\cdots a_n x_0^n = y_0\\\\\np(x_1)&amp;=a_0 + a_1 x_1 + a_2 x_1^2 + \\cdots a_n x_1^n = y_1\\\\\np(x_2)&amp;=a_0 + a_1 x_2 + a_2 x_2^2 + \\cdots a_n x_2^n = y_2\\\\\n\\cdots \\\\\np(x_n)&amp;=a_0 + a_1 x_n + a_2 x_n^2 + \\cdots a_n x_n^n = y_n\\\\\n\\end{aligned}\\]\n<p>We want to solve for the coefficients a_i. This is a linear\nsystem of equations that can be represented by the following matrix\nequation:</p>\n\\[{\\renewcommand{\\arraystretch}{1.5}\\begin{bmatrix}\n     1 &amp; x_0 &amp; x_0^2 &amp; \\dots &amp; x_0^n\\\\\n     1 &amp; x_1 &amp; x_1^2 &amp; \\dots &amp; x_1^n\\\\\n     1 &amp; x_2 &amp; x_2^2 &amp; \\dots &amp; x_2^n\\\\\n     \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots \\\\\n     1 &amp; x_n &amp; x_n^2 &amp; \\dots &amp; x_n^n\n \\end{bmatrix}\n \\begin{bmatrix}\n     a_0\\\\\n     a_1\\\\\n     a_2\\\\\n     \\vdots\\\\\n     a_n\\\\\n \\end{bmatrix}=\n \\begin{bmatrix}\n     y_0\\\\\n     y_1\\\\\n     y_2\\\\\n     \\vdots\\\\\n     y_n\\\\\n \\end{bmatrix}\n }\\]\n<p>The matrix on the left is called the <em>Vandermonde matrix</em>. This matrix\nis known to be invertible (see Appendix for a proof); therefore, this\nsystem of equations has a single solution that can be calculated by\ninverting the matrix.</p>\n<p>In practice, however, the Vandermonde matrix is often numerically\nill-conditioned, so inverting it isn’t the best way to calculate exact\npolynomial coefficients. Several better methods exist.</p>\n</div>\n<div class="section" id="lagrange-polynomial">\n<h2>Lagrange Polynomial</h2>\n<p>Lagrange interpolation polynomials emerge from a simple, yet powerful\nidea. Let’s define the <em>Lagrange basis</em> functions l_i(x)\n(i \\in [0, n]) as follows, given our points (x_i, y_i):</p>\n\\[l_i(x) =\n\\begin{cases}\n    1      &amp; x = x_i \\\\\n    0      &amp; x = x_j \\quad \\forall j \\neq i\n\\end{cases}\\]\n<p>In words, l_i(x) is constrained to 1 at <img alt="x_i" class="valign-m3" src="https://eli.thegreenplace.net/images/math/34e03e6559b14df9fe5a97bbd2ed10109dfebbd3.png" style="height: 11px;" /> and to 0 at\nall other x_j. We don’t care about its value at any other point.</p>\n<p>The linear combination:</p>\n\\[p(x)=\\sum_{i=0}^{n}y_i l_i(x)\\]\n<p>is then a valid interpolating polynomial for our set of n+1\npoints, because it’s equal to <img alt="y_i" class="valign-m4" src="https://eli.thegreenplace.net/images/math/35c2ac2f82d0ff8f9011b596ed7e54bfcc55f471.png" style="height: 12px;" /> at each <img alt="x_i" class="valign-m3" src="https://eli.thegreenplace.net/images/math/34e03e6559b14df9fe5a97bbd2ed10109dfebbd3.png" style="height: 11px;" /> (take a\nmoment to convince yourself this is true).</p>\n<p>How do we find l_i(x)? The key insight comes from studying the\nfollowing function:</p>\n\\[l&#x27;_i(x)=(x-x_0)\\cdot (x-x_1)\\cdots (x-x_{i-1}) \\cdot (x-x_{i+1})\\cdots (x-x_n)=\n\\prod_{\\substack{0\\leq j \\leq n \\\\ j \\neq i}}(x-x_j)\\]\n<p>This function has <img alt="n" class="valign-0" src="https://eli.thegreenplace.net/images/math/d1854cae891ec7b29161ccaf79a24b00c274bdaa.png" style="height: 8px;" /> terms (x-x_j) for all\nj\\neq i. It should be easy to see that l&#x27;_i(x) is 0 at\nall x_j when j\\neq i.</p>\n<p>What about its value at <img alt="x_i" class="valign-m3" src="https://eli.thegreenplace.net/images/math/34e03e6559b14df9fe5a97bbd2ed10109dfebbd3.png" style="height: 11px;" />, though? We can just assign\n<img alt="x_i" class="valign-m3" src="https://eli.thegreenplace.net/images/math/34e03e6559b14df9fe5a97bbd2ed10109dfebbd3.png" style="height: 11px;" /> into l&#x27;_i(x) to get:</p>\n\\[l&#x27;_i(x_i)=\\prod_{\\substack{0\\leq j \\leq n \\\\ j \\neq i}}(x_i-x_j)\\]\n<p>And then normalize l&#x27;_i(x), dividing it by this (constant) value. We get\nthe Lagrange basis function l_i(x):</p>\n\\[l_i(x)=\\frac{l&#x27;_i(x)}{l&#x27;_i(x_i)}=\\prod_{\\substack{0\\leq j \\leq n \\\\ j \\neq i}}\\frac{x-x_j}{x_i-x_j}\\]\n<p>Let’s use a concrete example to visualize this. Suppose we have the\nfollowing set of points we want to interpolate:\n(1,4), (2,2), (3,3). We can calculate l&#x27;_0(x),\nl&#x27;_1(x) and l&#x27;_2(x), and get the following:</p>\n<img alt="Un-normalized lagrange basis functions for our sample" class="align-center" src="https://eli.thegreenplace.net/images/2026/lagrange-basis.png" />\n<p>Note where each l&#x27;_i(x) intersects the <img alt="x" class="valign-0" src="https://eli.thegreenplace.net/images/math/11f6ad8ec52a2984abaafd7c3b516503785c2072.png" style="height: 8px;" /> axis. These\nfunctions have the right values at all x_{j\\neq i}. If we\nnormalize them to obtain l_i(x), we get these functions:</p>\n<img alt="Normalized lagrange basis functions for our sample" class="align-center" src="https://eli.thegreenplace.net/images/2026/lagrange-basis-normalized.png" />\n<p>Note that each polynomial is 1 at the appropriate <img alt="x_i" class="valign-m3" src="https://eli.thegreenplace.net/images/math/34e03e6559b14df9fe5a97bbd2ed10109dfebbd3.png" style="height: 11px;" /> and 0 at\nall the other x_{j\\neq i}, as required.</p>\n<p>With these l_i(x), we can now plot the interpolating polynomial\np(x)=\\sum_{i=0}^{n}y_i l_i(x), which fits our set of input points:</p>\n<img alt="Interpolation polynomial" class="align-center" src="https://eli.thegreenplace.net/images/2026/lagrange-inter-poly.png" />\n</div>\n<div class="section" id="polynomial-degree-and-uniqueness">\n<h2>Polynomial degree and uniqueness</h2>\n<p>We’ve just seen that the linear combination of Lagrange basis functions:</p>\n\\[p(x)=\\sum_{i=0}^{n}y_i l_i(x)\\]\n<p>is a valid interpolating polynomial for a set of n+1 distinct\npoints (x_i, y_i). What is its degree?</p>\n<p>Since the degree of each l_i(x) is <img alt="n" class="valign-0" src="https://eli.thegreenplace.net/images/math/d1854cae891ec7b29161ccaf79a24b00c274bdaa.png" style="height: 8px;" />, then the degree of\np(x) is <em>at most</em> <img alt="n" class="valign-0" src="https://eli.thegreenplace.net/images/math/d1854cae891ec7b29161ccaf79a24b00c274bdaa.png" style="height: 8px;" />. We’ve just derived the first part\nof the <em>Polynomial interpolation theorem</em>:</p>\n<p><strong>Polynomial interpolation theorem</strong>: for any n+1 data points\n(x_0,y_0), (x_1, y_1)\\cdots(x_n, y_n) \\in \\mathbb{R}^2 where no\ntwo x_j are the same, there exists a unique polynomial\np(x) of degree at most <img alt="n" class="valign-0" src="https://eli.thegreenplace.net/images/math/d1854cae891ec7b29161ccaf79a24b00c274bdaa.png" style="height: 8px;" /> that interpolates these points.</p>\n<p>We’ve demonstrated existence and degree, but not yet <em>uniqueness</em>. So\nlet’s turn to that.</p>\n<p>We know that p(x) interpolates all n+1 points, and its\ndegree is <img alt="n" class="valign-0" src="https://eli.thegreenplace.net/images/math/d1854cae891ec7b29161ccaf79a24b00c274bdaa.png" style="height: 8px;" />. Suppose there’s another such polynomial\nq(x). Let’s construct:</p>\n\\[r(x)=p(x)-r(x)\\]\n<p>That do we know about r(x)? First of all, its value is 0 at all\nour <img alt="x_i" class="valign-m3" src="https://eli.thegreenplace.net/images/math/34e03e6559b14df9fe5a97bbd2ed10109dfebbd3.png" style="height: 11px;" />, so it has n+1 <em>roots</em>. Second, we also know\nthat its degree is at most <img alt="n" class="valign-0" src="https://eli.thegreenplace.net/images/math/d1854cae891ec7b29161ccaf79a24b00c274bdaa.png" style="height: 8px;" /> (because it’s the difference of two\npolynomials of such degree). These two facts are a contradiction.\nNo non-zero polynomial of degree \\leq n can have\nn+1 roots (a basic algebraic fact related to the <em>Fundamental\ntheorem of algebra</em>). So r(x) must be the zero polynomial; in\nother words, our p(x) is unique \\blacksquare.</p>\n<p>Note the implication of uniqueness here: given our set of n+1\ndistinct points, there’s only one polynomial of degree \\leq n\nthat interpolates it. We can find its coefficients by inverting the\nVandermonde matrix, by using Lagrange basis functions, or\nany other method <a class="footnote-reference" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-2" id="footnote-reference-2">[2]</a>.</p>\n</div>\n<div class="section" id="lagrange-polynomials-as-a-basis-for-p-n-mathbb-r">\n<h2>Lagrange polynomials as a basis for P_n(\\mathbb{R})</h2>\n<p>The set P_n(\\mathbb{R}) consists of all real polynomials of\ndegree \\leq n. This set - along with addition of polynomials and\nscalar multiplication - <a class="reference external" href="https://eli.thegreenplace.net/2026/notes-on-linear-algebra-for-polynomials/">forms a vector\nspace</a>.</p>\n<p>We called l_i(x) the &quot;Lagrange basis&quot; previously, and they do -\nin fact - form an actual linear algebra basis for this vector space. To\nprove this claim, we need to show that Lagrange polynomials are linearly\nindependent and that they span the space.</p>\n<p><strong>Linear independence</strong>: we have to show that</p>\n\\[s(x)=\\sum_{i=0}^{n}a_i l_i(x)=0\\]\n<p>implies a_i=0 \\quad \\forall i. Recall that l_i(x) is 1\nat <img alt="x_i" class="valign-m3" src="https://eli.thegreenplace.net/images/math/34e03e6559b14df9fe5a97bbd2ed10109dfebbd3.png" style="height: 11px;" />, while all other l_j(x) are 0 at that point.\nTherefore, evaluating s(x) at <img alt="x_i" class="valign-m3" src="https://eli.thegreenplace.net/images/math/34e03e6559b14df9fe5a97bbd2ed10109dfebbd3.png" style="height: 11px;" />, we get:</p>\n\\[s(x_i)=a_i = 0\\]\n<p>Similarly, we can show that a_i is 0, for all <img alt="i" class="valign-0" src="https://eli.thegreenplace.net/images/math/042dc4512fa3d391c5170cf3aa61e6a638f84342.png" style="height: 12px;" />\n\\blacksquare.</p>\n<p><strong>Span</strong>: we’ve already demonstrated that the linear combination of\nl_i(x):</p>\n\\[p(x)=\\sum_{i=0}^{n}y_i l_i(x)\\]\n<p>is a valid interpolating polynomial for any set of n+1 distinct\npoints. Using the <em>polynomial interpolation theorem</em>, this is the unique\npolynomial interpolating this set of points. In other words, for every\nq(x)\\in P_n(\\mathbb{R}), we can identify any set of n+1 distinct points it passes\nthrough, and then use the technique described in this post to find the coefficients of q(x) in the\nLagrange basis. Therefore, the set l_i(x) spans\nthe vector space \\blacksquare.</p>\n</div>\n<div class="section" id="interpolation-matrix-in-the-lagrange-basis">\n<h2>Interpolation matrix in the Lagrange basis</h2>\n<p>Previously we’ve seen how to use the \\{1, x, x^2, \\dots x^n\\}\nbasis to write down a system of linear equations that helps us find the\ninterpolating polynomial. This results in the <em>Vandermonde matrix</em>.</p>\n<p>Using the Lagrange basis, we can get a much nicer matrix representation\nof the interpolation equations.</p>\n<p>Recall that our general polynomial using the Lagrange basis is:</p>\n\\[p(x)=\\sum_{i=0}^{n}a_i l_i(x)\\]\n<p>Let’s build a system of equations for each of the n+1 points\n(x_i,y_i). For <img alt="x_0" class="valign-m3" src="https://eli.thegreenplace.net/images/math/efbda784ad565c1c5201fdc948a570d0426bc6e6.png" style="height: 11px;" />:</p>\n\\[p(x_0)=\\sum_{i=0}^{n}a_i l_i(x_0)\\]\n<p>By definition of the Lagrange basis functions, all l_i(x_0)\nwhere i\\neq 0 are 0, while l_0(x_0) is 1. So this\nsimplifies to:</p>\n\\[p(x_0)=a_0\\]\n<p>But the value at node <img alt="x_0" class="valign-m3" src="https://eli.thegreenplace.net/images/math/efbda784ad565c1c5201fdc948a570d0426bc6e6.png" style="height: 11px;" /> is <img alt="y_0" class="valign-m4" src="https://eli.thegreenplace.net/images/math/2bb5817d0f3bf8490a8c7b1343f84f9635e683a3.png" style="height: 12px;" />, so we’ve just found\nthat a_0=y_0. We can produce similar equations for the other\nnodes as well, p(x_1)=a_1, etc. In matrix form:</p>\n\\[{\\renewcommand{\\arraystretch}{1.5}\\begin{bmatrix}\n     1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0\\\\\n     1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0\\\\\n     1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0\\\\\n     \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots \\\\\n     1 &amp; 0 &amp; 0 &amp; \\dots &amp; 1\n \\end{bmatrix}\n \\begin{bmatrix}\n     a_0\\\\\n     a_1\\\\\n     a_2\\\\\n     \\vdots\\\\\n     a_n\\\\\n \\end{bmatrix}=\n \\begin{bmatrix}\n     y_0\\\\\n     y_1\\\\\n     y_2\\\\\n     \\vdots\\\\\n     y_n\\\\\n \\end{bmatrix}\n }\\]\n<p>We get the identity matrix; this is another way to trivially show that\na_0=y_0, a_1=y_1 and so on.</p>\n</div>\n<div class="section" id="appendix-vandermonde-matrix">\n<h2>Appendix: Vandermonde matrix</h2>\n<p>Given some numbers \\{x_0 \\dots x_n\\} a matrix of this form:</p>\n\\[V=\n{\\renewcommand{\\arraystretch}{1.5}\\begin{bmatrix}\n1 &amp; x_0 &amp; x_0^2 &amp; \\dots &amp; x_0^n\\\\\n1 &amp; x_1 &amp; x_1^2 &amp; \\dots &amp; x_1^n\\\\\n1 &amp; x_2 &amp; x_2^2 &amp; \\dots &amp; x_2^n\\\\\n\\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots \\\\\n1 &amp; x_n &amp; x_n^2 &amp; \\dots &amp; x_n^n\n\\end{bmatrix}\n}\\]\n<p>Is called the <em>Vandermonde</em> matrix. What’s special about a Vandermonde\nmatrix is that we know it’s invertible when <img alt="x_i" class="valign-m3" src="https://eli.thegreenplace.net/images/math/34e03e6559b14df9fe5a97bbd2ed10109dfebbd3.png" style="height: 11px;" /> are distinct.\nThis is <a class="reference external" href="https://mathworld.wolfram.com/InvertibleMatrixTheorem.html">because its determinant is known to be\nnon-zero</a>.\nMoreover, its determinant is <a class="footnote-reference" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-3" id="footnote-reference-3">[3]</a>:</p>\n\\[\\det(V) = \\prod_{0 \\le i &lt; j \\le n} (x_j - x_i)\\]\n<p>Here’s why.</p>\n<p>To get some intuition, let’s consider some small-rank Vandermonde\nmatrices. Starting with a 2-by-2:</p>\n\\[\\det(V)=\\det\\begin{bmatrix}\n1 &amp; x_0 \\\\\n1 &amp; x_1 \\\\\n\\end{bmatrix}=x_1-x_0\\]\n<p>Let’s try 3-by-3 now:</p>\n\\[\\det(V)=\\det\n{\\renewcommand{\\arraystretch}{1.5}\\begin{bmatrix}\n    1 &amp; x_0 &amp; x_0^2 \\\\\n    1 &amp; x_1 &amp; x_1^2 \\\\\n    1 &amp; x_2 &amp; x_2^2 \\\\\n\\end{bmatrix}\n}\\]\n<p>We can use the standard way of calculating determinants to expand from\nthe first row:</p>\n\\[\\begin{aligned}\n\\det(V)&amp;=1\\cdot(x_1 x_2^2 - x_2 x_1^2)-x_0(x_2^2-x_1^2)+x_0^2(x_2 - x_1)\\\\\n&amp;=x_1 x_2^2 - x_2 x_1^2 - x_0 x_2^2+x_0 x_1^2+x_0^2 x_2 - x_0^2 x_1\\\\\n\\end{aligned}\\]\n<p>Using some algebraic manipulation, it’s easy to show this is equivalent\nto:</p>\n\\[\\det(V)=(x_2-x_1)(x_2-x_0)(x_1-x_0)\\]\n<p>For the full proof, let’s look at the generalized\nn+1-by-n+1 matrix again:</p>\n\\[V=\n{\\renewcommand{\\arraystretch}{1.5}\\begin{bmatrix}\n        1 &amp; x_0 &amp; x_0^2 &amp; \\dots &amp; x_0^n\\\\\n        1 &amp; x_1 &amp; x_1^2 &amp; \\dots &amp; x_1^n\\\\\n        1 &amp; x_2 &amp; x_2^2 &amp; \\dots &amp; x_2^n\\\\\n        \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots \\\\\n        1 &amp; x_n &amp; x_n^2 &amp; \\dots &amp; x_n^n\n    \\end{bmatrix}\n }\\]\n<p>Recall that subtracting a multiple of one column from another doesn’t\nchange a matrix’s determinant. For each column k&gt;1, we’ll\nsubtract the value of column k-1 multiplied by <img alt="x_0" class="valign-m3" src="https://eli.thegreenplace.net/images/math/efbda784ad565c1c5201fdc948a570d0426bc6e6.png" style="height: 11px;" /> from\nit (this is done on all columns simultaneously). The idea is to make the\nfirst row all zeros after the very first element:</p>\n\\[V=\n{\\renewcommand{\\arraystretch}{1.5}\\begin{bmatrix}\n        1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0\\\\\n        1 &amp; x_1 - x_0 &amp; x_1^2 - x_1 x_0&amp; \\dots &amp; x_1^n - x_1^{n-1} x_0\\\\\n        1 &amp; x_2 - x_0 &amp; x_2^2 - x_2 x_0&amp; \\dots &amp; x_2^n - x_2^{n-1} x_0\\\\\n        \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots \\\\\n        1 &amp; x_n - x_0 &amp; x_n^2 - x_n x_0&amp; \\dots &amp; x_n^n - x_n^{n-1} x_0\\\\\n    \\end{bmatrix}\n}\\]\n<p>Now we factor out x_1-x_0 from the second row (after the first\nelement), x_2-x_0 from the third row and so on, to get:</p>\n\\[V=\n{\\renewcommand{\\arraystretch}{1.5}\\begin{bmatrix}\n        1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0\\\\\n        1 &amp; x_1 - x_0 &amp; x_1(x_1 - x_0)&amp; \\dots &amp; x_1^{n-1}(x_1 - x_0)\\\\\n        1 &amp; x_2 - x_0 &amp; x_2(x_2 - x_0)&amp; \\dots &amp; x_2^{n-1}(x_2 - x_0)\\\\\n        \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots \\\\\n        1 &amp; x_n - x_0 &amp; x_n(x_n - x_0)&amp; \\dots &amp; x_n^{n-1}(x_n - x_0)\\\\\n    \\end{bmatrix}\n}\\]\n<p>Imagine we erase the first row and first column of <img alt="V" class="valign-0" src="https://eli.thegreenplace.net/images/math/c9ee5681d3c59f7541c27a38b67edf46259e187b.png" style="height: 12px;" />. We’ll call\nthe resulting matrix <img alt="W" class="valign-0" src="https://eli.thegreenplace.net/images/math/e2415cb7f63df0c9de23362326ad3c37a9adfc96.png" style="height: 12px;" />.</p>\n\\[W=\n{\\renewcommand{\\arraystretch}{1.5}\\begin{bmatrix}\n        x_1 - x_0 &amp; x_1(x_1 - x_0)&amp; \\dots &amp; x_1^{n-1}(x_1 - x_0)\\\\\n        x_2 - x_0 &amp; x_2(x_2 - x_0)&amp; \\dots &amp; x_2^{n-1}(x_2 - x_0)\\\\\n        \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots \\\\\n        x_n - x_0 &amp; x_n(x_n - x_0)&amp; \\dots &amp; x_n^{n-1}(x_n - x_0)\\\\\n    \\end{bmatrix}\n}\\]\n<p>Because the first row of <img alt="V" class="valign-0" src="https://eli.thegreenplace.net/images/math/c9ee5681d3c59f7541c27a38b67edf46259e187b.png" style="height: 12px;" /> is all zeros except the first\nelement, we have:</p>\n\\[\\det(V)=\\det(W)\\]\n<p>Note that the first row of <img alt="W" class="valign-0" src="https://eli.thegreenplace.net/images/math/e2415cb7f63df0c9de23362326ad3c37a9adfc96.png" style="height: 12px;" /> has a common factor of\nx_1-x_0, so when calculating \\det(W), we can move this\ncommon factor out. Same for the common factor x_2-x_0 of the\nsecond row, and so on. Overall, we can write:</p>\n\\[\\det(W)=(x_1-x_0)(x_2-x_0)\\cdots(x_n-x_0)\\cdot \\det\n{\\renewcommand{\\arraystretch}{1.5}\\begin{bmatrix}\n        1 &amp; x_0 &amp; x_0^2 &amp; \\dots &amp; x_0^{n-1}\\\\\n        1 &amp; x_1 &amp; x_1^2 &amp; \\dots &amp; x_1^{n-1}\\\\\n        1 &amp; x_2 &amp; x_2^2 &amp; \\dots &amp; x_2^{n-1}\\\\\n        \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots \\\\\n        1 &amp; x_n &amp; x_n^2 &amp; \\dots &amp; x_n^{n-1}\n    \\end{bmatrix}\n}\\]\n<p>But the smaller matrix is just the Vandermonde matrix for\n\\{x_0 \\dots x_{n-1}\\}. If we continue this process by induction,\nwe’ll get:</p>\n\\[\\det(V) = \\prod_{0 \\le i &lt; j \\le n} (x_j - x_i)\\]\n<p>If you’re interested, the <a class="reference external" href="https://en.wikipedia.org/wiki/Vandermonde_matrix">Wikipedia page for the Vandermonde matrix</a> has a couple of additional\nproofs.</p>\n<hr class="docutils" />\n<table class="docutils footnote" frame="void" id="footnote-1" rules="none">\n<colgroup><col class="label" /><col /></colgroup>\n<tbody valign="top">\n<tr><td class="label"><a class="fn-backref" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-reference-1">[1]</a></td><td>The <img alt="x" class="valign-0" src="https://eli.thegreenplace.net/images/math/11f6ad8ec52a2984abaafd7c3b516503785c2072.png" style="height: 8px;" />-es here are called <em>nodes</em> and the <img alt="y" class="valign-m4" src="https://eli.thegreenplace.net/images/math/95cb0bfd2977c761298d9624e4b4d4c72a39974a.png" style="height: 12px;" />-s are\ncalled <em>values</em>.</td></tr>\n</tbody>\n</table>\n<table class="docutils footnote" frame="void" id="footnote-2" rules="none">\n<colgroup><col class="label" /><col /></colgroup>\n<tbody valign="top">\n<tr><td class="label"><a class="fn-backref" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-reference-2">[2]</a></td><td><a class="reference external" href="https://eli.thegreenplace.net/2024/method-of-differences-and-newton-polynomials/">Newton\npolynomials</a>\nis also an option, and there are many other approaches.</td></tr>\n</tbody>\n</table>\n<table class="docutils footnote" frame="void" id="footnote-3" rules="none">\n<colgroup><col class="label" /><col /></colgroup>\n<tbody valign="top">\n<tr><td class="label"><a class="fn-backref" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-reference-3">[3]</a></td><td>Note that this means the product of all differences between\nx_j and <img alt="x_i" class="valign-m3" src="https://eli.thegreenplace.net/images/math/34e03e6559b14df9fe5a97bbd2ed10109dfebbd3.png" style="height: 11px;" /> where <img alt="i" class="valign-0" src="https://eli.thegreenplace.net/images/math/042dc4512fa3d391c5170cf3aa61e6a638f84342.png" style="height: 12px;" /> is strictly smaller than\nj. That is, for n=2, the full product is\n(x_2-x_1)(x_2-x_0)(x_1-x_0). For an arbitrary <img alt="n" class="valign-0" src="https://eli.thegreenplace.net/images/math/d1854cae891ec7b29161ccaf79a24b00c274bdaa.png" style="height: 8px;" />,\nthere are \\frac{n(n-1)}{2} factors in total.</td></tr>\n</tbody>\n</table>\n</div>'}

---

*抓取时间: 2026-03-01 12:06:12*
