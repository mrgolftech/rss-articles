# Notes on Linear Algebra for Polynomials

**来源:** [eli.thegreenplace.net](https://eli.thegreenplace.net)
**发布时间:** 2026-02-25T18:34:00-08:00
**链接:** https://eli.thegreenplace.net/2026/notes-on-linear-algebra-for-polynomials/

---

{'type': 'text/html', 'language': None, 'base': 'https://eli.thegreenplace.net/feeds/all.atom.xml', 'value': '<p>We’ll be working with the set P_n(\\mathbb{R}), real polynomials\nof degree \\leq n. Such polynomials can be expressed using\nn+1 scalar coefficients a_i as follows:</p>\n\\[p(x)=a_0+a_1 x + a_2 x^2 + \\cdots + a_n x^n\\]\n<div class="section" id="vector-space">\n<h2>Vector space</h2>\n<p>The set P_n(\\mathbb{R}), along with addition of polynomials and\nscalar multiplication form a <em>vector space</em>. As a proof, let’s review\nhow the vector space axioms are satisfied. We’ll use p(x),\nq(x) and r(x) as arbitrary polynomials from the set\nP_n(\\mathbb{R}) for the demonstration. Similarly, a and\nb are arbitrary scalars in <img alt="\\mathbb{R}" class="valign-0" src="https://eli.thegreenplace.net/images/math/0ed839b111fe0e3ca2b2f618b940893eaea88a57.png" style="height: 12px;" />.</p>\n<p><strong>Associativity of vector addition</strong>:</p>\n\\[p(x)+[q(x)+r(x)]=p(x)+q(x)+r(x)=[p(x)+q(x)]+r(x)\\]\n<p>This is trivial because addition of polynomials is associative  <a class="footnote-reference" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-1" id="footnote-reference-1">[1]</a>.\nCommutativity is similarly trivial, for the same reason:</p>\n<p><strong>Commutativity of vector addition</strong>:</p>\n\\[p(x)+q(x)=q(x)+p(x)\\]\n<p><strong>Identity element of vector addition</strong>:</p>\n<p>The zero polynomial 0 serves as an identity element.\n\\forall p(x)\\in P_n(\\mathbb{R}), we have\n0 + p(x) = p(x).</p>\n<p><strong>Inverse element of vector addition</strong>:</p>\n<p>For each p(x), we can use q(x)=-p(x) as the additive\ninverse, because p(x)+q(x)=0.</p>\n<p><strong>Identity element of scalar multiplication</strong></p>\n<p>The scalar 1 serves as an identity element for scalar multiplication.\nFor each p(x), it’s true that 1\\cdot p(x)=p(x).</p>\n<p><strong>Associativity of scalar multiplication</strong>:</p>\n<p>For any two scalars a and b:</p>\n\\[a[b\\cdot p(x)]=ab\\cdot p(x)=[ab]\\cdot p(x)\\]\n<p><strong>Distributivity of scalar multiplication over vector addition</strong>:</p>\n<p>For any p(x), q(x) and scalar a:</p>\n\\[a\\cdot[p(x)+q(x)]=a\\cdot p(x)+a\\cdot q(x)\\]\n<p><strong>Distributivity of scalar multiplication over scalar addition</strong>:</p>\n<p>For any scalars a and b and polynomial p(x):</p>\n\\[[a+b]\\cdot p(x)=a\\cdot p(x) + b\\cdot p(x)\\]\n</div>\n<div class="section" id="linear-independence-span-and-basis">\n<h2>Linear independence, span and basis</h2>\n<p>Since we’ve shown that polynomials in P_n(\\mathbb{R}) form a\nvector space, we can now build additional linear algebraic definitions\non top of that.</p>\n<p>A set of k polynomials p_k(x)\\in P_n(\\mathbb{R}) is said\nto be <em>linearly independent</em> if</p>\n\\[\\sum_{i=1}^{k}a_i p_i(x)=0\\]\n<p>implies a_i=0 \\quad \\forall i. In words, the only linear\ncombination resulting in the zero vector is when all coefficients are 0.</p>\n<p>As an example, let’s discuss the fundamental building blocks of\npolynomials in P_n(\\mathbb{R}): the set\n\\{1, x, x^2, \\dots x^n\\}. These are linearly independent\nbecause:</p>\n\\[a_0 + a_1 x + a_2 x^2 + \\cdots a_n x^n=0\\]\n<p>is true only for zero polynomial, in which all the coefficients\na_i=0. This comes from the very definition of polynomials.\nMoreover, this set <em>spans</em> the entire P_n(\\mathbb{R}) because\nevery polynomial can be (by definition) expressed as a linear combination of\n\\{1, x, x^2, \\dots x^n\\}.</p>\n<p>Since we’ve shown these basic polynomials are linearly independent and\nspan the entire vector space, they are a <em>basis</em> for the space. In fact,\nthis set has a special name: the <em>monomial basis</em> (because a monomial is\na polynomial with a single term).</p>\n</div>\n<div class="section" id="checking-if-an-arbitrary-set-of-polynomials-is-a-basis">\n<h2>Checking if an arbitrary set of polynomials is a basis</h2>\n<p>Suppose we have some set polynomials, and we want to know if these form\na basis for P_n(\\mathbb{R}). How do we go about it?</p>\n<p>The idea is using linear algebra the same way we do for any other vector\nspace. Let’s use a concrete example to demonstrate:</p>\n\\[Q=\\{1-x, x, 2x+x^2\\}\\]\n<p>Is the set Q a basis for P_n(\\mathbb{R})? We’ll start by\nchecking whether the members of Q are linearly independent.\nWrite:</p>\n\\[a_0(1-x)+a_1 x + a_2(2x+x^2)=0\\]\n<p>By regrouping, we can turn this into:</p>\n\\[a_0 + (a_1-a_0+2a_2)x+a_2 x^2=0\\]\n<p>For this to be true, the coefficient of each monomial has to be zero;\nmathematically:</p>\n\\[\\begin{aligned}\n    a_0&amp;=0\\\\\n    a_1-a_0+2a_2&amp;=0\\\\\n    a2&amp;=0\\\\\n\\end{aligned}\\]\n<p>In matrix form:</p>\n\\[\\begin{bmatrix}\n    1 &amp; 0 &amp; 0\\\\\n    -1 &amp; 1 &amp; 2\\\\\n    0 &amp; 0 &amp; 1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}a_0\\\\ a_1\\\\ a_2\\end{bmatrix}\n=\\begin{bmatrix}0\\\\ 0\\\\ 0\\end{bmatrix}\\]\n<p>We know how to solve this, by reducing the matrix into <a class="reference external" href="https://en.wikipedia.org/wiki/Row_echelon_form">row-echelon\nform</a>. It’s easy to\nsee that the reduced row-echelon form of this specific matrix is\nI, the identity matrix. Therefore, this set of equations has a\nsingle solution: a_i=0 \\quad \\forall i  <a class="footnote-reference" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-2" id="footnote-reference-2">[2]</a>.</p>\n<p>We’ve shown that the set Q is linearly independent. Now let’s\nshow that it <em>spans</em> the space P_n(\\mathbb{R}). We want to\nanalyze:</p>\n\\[a_0(1-x)+a_1 x + a_2(2x+x^2)=\\alpha +\\beta x + \\gamma x^2\\]\n<p>And find the coefficients a_i that satisfy this for any\narbitrary <img alt="\\alpha" class="valign-0" src="https://eli.thegreenplace.net/images/math/f7c665b45932a814215e979bc2611080b4948e68.png" style="height: 8px;" />, <img alt="\\beta" class="valign-m4" src="https://eli.thegreenplace.net/images/math/6499d503bfc00cadae1440b191c52a8632e2f8c4.png" style="height: 16px;" /> and \\gamma. We proceed\njust as before, by regrouping on the left side:</p>\n\\[a_0 + (a_1-a_0+2a_2)x+a_2 x^2=\\alpha +\\beta x + \\gamma x^2\\]\n<p>and equating the coefficient of each power of <img alt="x" class="valign-0" src="https://eli.thegreenplace.net/images/math/11f6ad8ec52a2984abaafd7c3b516503785c2072.png" style="height: 8px;" /> separately:</p>\n\\[\\begin{aligned}\n    a_0&amp;=\\alpha\\\\\n    a_1-a_0+2a_2&amp;=\\beta\\\\\n    a2&amp;=\\gamma\\\\\n\\end{aligned}\\]\n<p>If we turn this into matrix form, the matrix of coefficients is exactly\nthe same as before. So we know there’s a single solution, and by\nrearranging the matrix into I, the solution will appear on the\nright hand side. It doesn’t matter for the moment what the actual\nsolution is, as long as it exists and is unique. We’ve shown that\nQ spans the space!</p>\n<p>Since the set Q is linearly independent and spans\nP_n(\\mathbb{R}), it is a <em>basis</em> for the space.</p>\n</div>\n<div class="section" id="inner-product">\n<h2>Inner product</h2>\n<p>I’ve discussed inner products for functions in <a class="reference external" href="https://eli.thegreenplace.net/2025/hilbert-space-treating-functions-as-vectors/">the post about Hilbert\nspace</a>.\nWell, <em>polynomials are functions</em>, so we can define an inner product\nusing integrals as follows <a class="footnote-reference" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-3" id="footnote-reference-3">[3]</a>:</p>\n\\[\\langle p, q \\rangle = \\int_{a}^{b} p(x) q(x) w(x) \\, dx\\]\n<p>Where the bounds a and b are arbitrary, and could be\ninfinite. Whenever we deal with integrals we worry about convergence; in\nmy post on Hilbert spaces, we only talked about L^2 - the square\nintegrable functions. Most polynomials are not square integrable,\nhowever. Therefore, we can restrict this using either:</p>\n<ol class="arabic simple">\n<li>A special <em>weight function</em> w(x) to make sure the inner\nproduct integral converges</li>\n<li>Set finite bounds on the integral, and then we can just set\nw(x)=1.</li>\n</ol>\n<p>Let’s use the latter, and restrict the bounds into the range\n[-1,1], setting w(x)=1. We have the following inner\nproduct:</p>\n\\[\\langle p, q \\rangle = \\int_{-1}^{1} p(x) q(x) \\, dx\\]\n<p>Let’s check that this satisfies the inner product space conditions.</p>\n<p><strong>Conjugate symmetry</strong>:</p>\n<p>Since real multiplication is commutative, we can write:</p>\n\\[\\langle p, q \\rangle = \\int_{-1}^{1} p(x) q(x) \\, dx =\\int_{-1}^{1} q(x) p(x) \\, dx=\\langle q, p \\rangle\\]\n<p>We deal in the reals here, so we can safely ignore complex conjugation.</p>\n<p><strong>Linearity in the first argument</strong>:</p>\n<p>Let p_1,p_2,q\\in P_n(\\mathbb{R}) and a,b\\in \\mathbb{R}.\nWe want to show that</p>\n\\[\\langle ap_1+bp_2,q \\rangle = a\\langle p_1,q\\rangle +b\\langle p_2,q\\rangle\\]\n<p>Expand the left-hand side using our definition of inner product:</p>\n\\[\\begin{aligned}\n    \\langle ap_1+bp_2,q \\rangle&amp;=\\int_{-1}^{1} (a p_1(x)+b p_2(x)) q(x) \\, dx\\\\\n    &amp;=a\\int_{-1}^{1} p_1(x) q(x) \\, dx+b\\int_{-1}^{1} p_2(x) q(x) \\, dx\n\\end{aligned}\\]\n<p>The result is equivalent to\na\\langle p_1,q\\rangle +b\\langle p_2,q\\rangle.</p>\n<p><strong>Positive-definiteness</strong>:</p>\n<p>We want to show that for nonzero p\\in P_n(\\mathbb{R}), we have\n\\langle p, p\\rangle &gt; 0. First of all, since p(x)^2\\geq0\nfor all <img alt="x" class="valign-0" src="https://eli.thegreenplace.net/images/math/11f6ad8ec52a2984abaafd7c3b516503785c2072.png" style="height: 8px;" />, it’s true that:</p>\n\\[\\langle p, p\\rangle=\\int_{-1}^{1}p(x)^2\\, dx\\geq 0\\]\n<p>What about the result 0 though? Well, let’s say that</p>\n\\[\\int_{-1}^{1}p(x)^2\\, dx=0\\]\n<p>Since p(x)^2 is a non-negative function, this means that the\nintegral of a non-negative function ends up being 0. But p(x) is\na polynomial, so it’s <em>continuous</em>, and so is p(x)^2. If the\nintegral of a continuous non-negative function is 0, it means the\nfunction itself is 0. Had it been non-zero in any place, the integral\nwould necessarily have to be positive as well.</p>\n<p>We’ve proven that \\langle p, p\\rangle=0 only when p is\nthe zero polynomial. The positive-definiteness condition is satisfied.</p>\n<p>In conclusion, P_n(\\mathbb{R}) along with the inner product\nwe’ve defined forms an <em>inner product space</em>.</p>\n</div>\n<div class="section" id="orthogonality">\n<h2>Orthogonality</h2>\n<p>Now that we have an inner product, we can define orthogonality on\npolynomials: two polynomials p,q are <em>orthogonal</em> (w.r.t. our\ninner product) iff</p>\n\\[\\langle p,q\\rangle=\\int_{-1}^{1}p(x)q(x)\\, dx=0\\]\n<p>Contrary to expectation  <a class="footnote-reference" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-4" id="footnote-reference-4">[4]</a>, the monomial basis polynomials are <em>not</em>\northogonal using our definition of inner product.</p>\n<p>For example, calculating the inner product for 1 and\nx^2:</p>\n\\[\\langle 1,x^2\\rangle=\\int_{-1}^{1}x^2\\, dx=\\frac{x^3}{3}\\biggr|_{-1}^{1}=\\frac{2}{3}\\]\n<p>There are other sets of polynomials that <em>are</em> orthogonal using our\ninner product. For example, the <a class="reference external" href="https://en.wikipedia.org/wiki/Legendre_polynomials">Legendre\npolynomials</a>; but\nthis is a topic for another post.</p>\n<hr class="docutils" />\n<table class="docutils footnote" frame="void" id="footnote-1" rules="none">\n<colgroup><col class="label" /><col /></colgroup>\n<tbody valign="top">\n<tr><td class="label"><a class="fn-backref" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-reference-1">[1]</a></td><td>There’s a level of basic algebra below which we won’t descend in\nthese notes. We could break this statement further down by saying\nthat something like a_i x^i + a_j x^j can be added to\nb_i x^i + b_j x^j by adding each power of <img alt="x" class="valign-0" src="https://eli.thegreenplace.net/images/math/11f6ad8ec52a2984abaafd7c3b516503785c2072.png" style="height: 8px;" />\nseparately for any <img alt="i" class="valign-0" src="https://eli.thegreenplace.net/images/math/042dc4512fa3d391c5170cf3aa61e6a638f84342.png" style="height: 12px;" /> and j, but let’s just take it\nfor granted.</td></tr>\n</tbody>\n</table>\n<table class="docutils footnote" frame="void" id="footnote-2" rules="none">\n<colgroup><col class="label" /><col /></colgroup>\n<tbody valign="top">\n<tr><td class="label"><a class="fn-backref" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-reference-2">[2]</a></td><td>Obviously, this specific set of equations is quite trivial to solve\nwithout matrices; I just want to demonstrate the more general\napproach. Once we have a system of linear equations, the whole\ntoolbox of linear algebra is at our disposal. For example, we could\nalso have checked the determinant and seen it’s non-zero, which means\nthat a square matrix is invertible, and in this case has a single\nsolution of zeroes.</td></tr>\n</tbody>\n</table>\n<table class="docutils footnote" frame="void" id="footnote-3" rules="none">\n<colgroup><col class="label" /><col /></colgroup>\n<tbody valign="top">\n<tr><td class="label"><a class="fn-backref" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-reference-3">[3]</a></td><td>And actually with this (or any valid) inner product,\nP_n(\\mathbb{R}) indeed forms a Hilbert space, because it’s\nfinite-dimensional, and every finite-dimensional inner product space\nis complete.</td></tr>\n</tbody>\n</table>\n<table class="docutils footnote" frame="void" id="footnote-4" rules="none">\n<colgroup><col class="label" /><col /></colgroup>\n<tbody valign="top">\n<tr><td class="label"><a class="fn-backref" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-reference-4">[4]</a></td><td>Because of how naturally this set spans P_n(\\mathbb{R}). And\nindeed, we can define alternative inner products using which\nmonomials are orthogonal.</td></tr>\n</tbody>\n</table>\n</div>'}

---

*抓取时间: 2026-02-27 02:38:51*
