# BREAKING: LLM “reasoning” continues to be deeply flawed

**来源:** [garymarcus.substack.com](https://garymarcus.substack.com)
**发布时间:** Tue, 10 Feb 2026 21:31:52 GMT
**链接:** https://garymarcus.substack.com/p/breaking-llm-reasoning-continues

---

{'type': 'text/html', 'language': None, 'base': 'https://garymarcus.substack.com/feed', 'value': '<p>As you may know, I have been harping on reasoning as a core challenge for deep learning for well over a decade, <a href="https://www.newyorker.com/news/news-desk/is-deep-learning-a-revolution-in-artificial-intelligence">at least since a December 2012 New Yorker article</a>:</p><blockquote><p>&#8220;Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (such as between diseases and their symptoms), and are <a href="http://smash.psych.nyu.edu/courses/spring09/modeling/materials/marcusrethink.pdf">likely to face challenges</a> in acquiring abstract ideas &#8230; they have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge&#8221;</p></blockquote><p>By now many, many others have pounded on the same point.  </p><p>Silicon Valley&#8217;s response has always been to dismiss us. We got this covered, the Valley CEOs will tell you. Pay no attention to Gary or any of the other academics from <a href="https://cotopaxi.eas.asu.edu/">Subbarao Kambhampati</a> to Judea Pearl to Ernest Davis to Ken Forbus to <span class="mention-wrap"></span> to Yann LeCun to Francesca Rossi, and many others (including more recently Ilya Sutskever), who are also skeptical. Ignore <a href="https://aaai.org/about-aaai/presidential-panel-on-the-future-of-ai-research/">the big AAAI survey that said that LLMs won&#8217;t get us to AGI</a>. Surely <a href="https://open.substack.com/pub/garymarcus/p/a-knockout-blow-for-llms?r=8tdk6&amp;utm_medium=ios">that Apple study</a> must be biased too. Give us money. Lots of money. Lots and lots of money. Scale Scale Scale. AGI is coming next year!</p><p>Well, AGI still hasn&#8217;t come (even though they keep issuing the same promises, year after year). LLMs still hallucinate and continue to make boneheaded errors.</p><p>And reasoning is still one of the core issues.</p><p>How serious an issue? A new review from Caltech and Stanford, called <a href="https://arxiv.org/abs/2602.06176">Large Language Model Reasoning Failure</a> shows <a href="https://arxiv.org/pdf/2602.06176">the latest deep learning systems &#8212; even the ones marketed as &#8220;reasoning&#8221; systems &#8212; (still) have major problems with reasoning</a>. </p><p>Lots and lots of problems, everywhere they looked:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/$s_!Qx03!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe538e23c-dfb5-4bdd-850a-ff155ea67c8b_1515x1253.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1204" src="https://substackcdn.com/image/fetch/$s_!Qx03!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe538e23c-dfb5-4bdd-850a-ff155ea67c8b_1515x1253.png" width="1456" /><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><button class="pencraft pc-reset pencraft icon-container restack-image" tabindex="0" type="button"><svg fill="none" height="20" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 20 20" width="20" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882"></path></g></svg></button><button class="pencraft pc-reset pencraft icon-container view-image" tabindex="0" type="button"><svg class="lucide lucide-maximize2 lucide-maximize-2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></button></div></div></div></a></figure></div><p>The paper is a thorough taxonomy/review, with excellent bibliography. </p><p>If you want a full idea of how far we have left to go (and, implicitly, why LLMs aren&#8217;t the right answer) read it. (Disclaimer: one of the paper&#8217;s authors, Noah Goodman, worked with me at my first startup Geometric Intelligence, which was acquired by Uber.)</p><p>Silicon Valley can do one of two things. It can continue to dismiss critics and criticism, praying for magical solutions, or it can face reality and start focusing on alternatives to LLMs.  </p><p>&#129335;&#8205;&#9794;&#65039;</p>'}

---

*抓取时间: 2026-02-11 06:02:39*
