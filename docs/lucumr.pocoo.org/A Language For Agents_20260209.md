# A Language For Agents

**来源:** [lucumr.pocoo.org](https://lucumr.pocoo.org)
**发布时间:** 2026-02-09T00:00:00+00:00
**链接:** https://lucumr.pocoo.org/2026/2/9/a-language-for-agents/

---

{'type': 'text/html', 'language': None, 'base': 'https://lucumr.pocoo.org/feed.atom', 'value': '<p>Last year I first started thinking about what the future of programming\nlanguages might look like now that agentic engineering is a growing thing.\nInitially I felt that the enormous corpus of pre-existing code would cement\nexisting languages in place but now I&#8217;m starting to think the opposite is true.\nHere I want to outline my thinking on why we are going to see more new\nprogramming languages and why there is quite a bit of space for interesting\ninnovation.  And just in case someone wants to start building one, here are some\nof my thoughts on what we should aim for!</p>\n<h2>Why New Languages Work</h2>\n<p>Does an agent perform dramatically better on a language that it has in its\nweights?  Obviously yes.  But there are less obvious factors that affect how\ngood an agent is at programming in a language: how good the tooling around it is\nand how much churn there is.</p>\n<p>Zig seems underrepresented in the weights (at least in the models I&#8217;ve used)\nand also changing quickly.  That combination is not optimal, but it&#8217;s still\npassable: you can program even in the upcoming Zig version if you point the\nagent at the right documentation.  But it&#8217;s not great.</p>\n<p>On the other hand, some languages are well represented in the weights but agents\nstill don&#8217;t succeed as much because of tooling choices.  Swift is a good\nexample: in my experience the tooling around building a Mac or iOS application\ncan be so painful that agents struggle to navigate it.  Also not great.</p>\n<p>So, just because it exists doesn&#8217;t mean the agent succeeds and just because it&#8217;s\nnew also doesn&#8217;t mean that the agent is going to struggle.  I&#8217;m convinced that\nyou can build yourself up to a new language if you don&#8217;t want to depart\neverywhere all at once.</p>\n<p>The biggest reason new languages might work is that the cost of coding is going\ndown dramatically.  The result is the breadth of an ecosystem matters less. I&#8217;m\nnow routinely reaching for JavaScript in places where I would have used Python.\nNot because I love it or the ecosystem is better, but because the agent does\nmuch better with TypeScript.</p>\n<p>The way to think about this: if important functionality is missing in my\nlanguage of choice, I just point the agent at a library from a different\nlanguage and have it build a port.  As a concrete example, I recently built an\nEthernet driver in JavaScript to implement the host controller for our sandbox.\nImplementations exist in Rust, C, and Go, but I wanted something pluggable and\ncustomizable in JavaScript.  It was easier to have the agent reimplement it than\nto make the build system and distribution work against a native binding.</p>\n<p>New languages will work if their value proposition is strong enough and they\nevolve with knowledge of how LLMs train.  People will adopt them despite being\nunderrepresented in the weights.  And if they are designed to work well with\nagents, then they might be designed around familiar syntax that is already known\nto work well.</p>\n<h2>Why A New Language?</h2>\n<p>So why would we want a new language at all?  The reason this is interesting to\nthink about is that many of today&#8217;s languages were designed with the assumption\nthat punching keys is laborious, so we traded certain things for brevity.  As an\nexample, many languages — particular modern ones — lean heavily on type\ninference so that you don&#8217;t have to write out types.  The downside is that you\nnow need an LSP or the resulting compiler error messages to figure out what the\ntype of an expression is.  Agents struggle with this too, and it&#8217;s also\nfrustrating in pull request review where complex operations can make it very\nhard to figure out what the types actually are.  Fully dynamic languages are\neven worse in that regard.</p>\n<p>The cost of writing code is going down, but because we are also producing more\nof it, understanding what the code does is becoming more important.  We might\nactually want more code to be written if it means there is less ambiguity when\nwe perform a review.</p>\n<p>I also want to point out that we are heading towards a world where some code is\nnever seen by a human and is only consumed by machines.  Even in that case, we\nstill want to give an indication to a user, who is potentially a non-programmer,\nabout what is going on.  We want to be able to explain to a user what the code\nwill do without going into the details of how.</p>\n<p>So the case for a new language comes down to: given the fundamental changes in\nwho is programming and what the cost of code is, we should at least consider\none.</p>\n<h2>What Agents Want</h2>\n<p>It&#8217;s tricky to say what an agent wants because agents will lie to you and they\nare influenced by all the code they&#8217;ve seen.  But one way to estimate how they\nare doing is to look at how many changes they have to perform on files and how\nmany iterations they need for common tasks.</p>\n<p>There are some things I&#8217;ve found that I think will be true for a while.</p>\n<h3>Context Without LSP</h3>\n<p>The language server protocol lets an IDE infer information about what&#8217;s under\nthe cursor or what should be autocompleted based on semantic knowledge of the\ncodebase.  It&#8217;s a great system, but it comes at one specific cost that is tricky\nfor agents: the LSP has to be running.</p>\n<p>There are situations when an agent just won&#8217;t run the LSP — not because of\ntechnical limitations, but because it&#8217;s also lazy and will skip that step if it\ndoesn&#8217;t have to.  If you give it an example from documentation, there is no easy\nway to run the LSP because it&#8217;s a snippet that might not even be complete.  If\nyou point it at a GitHub repository and it pulls down individual files, it will\njust look at the code.  It won&#8217;t set up an LSP for type information.</p>\n<p>A language that doesn&#8217;t split into two separate experiences (with-LSP and\nwithout-LSP) will be beneficial to agents because it gives them one unified way\nof working across many more situations.</p>\n<h3>Braces, Brackets, and Parentheses</h3>\n<p>It pains me as a Python developer to say this, but whitespace-based indentation\nis a problem.  The underlying token efficiency of getting whitespace right is\ntricky, and a language with significant whitespace is harder for an LLM to work\nwith.  This is particularly noticeable if you try to make an LLM do surgical\nchanges without an assisted tool.  Quite often they will intentionally disregard\nwhitespace, add markers to enable or disable code and then rely on a code\nformatter to clean up indentation later.</p>\n<p>On the other hand, braces that are not separated by whitespace can cause issues\ntoo.  Depending on the tokenizer, runs of closing parentheses can end up split\ninto tokens in surprising ways (a bit like the &#8220;strawberry&#8221; counting problem),\nand it&#8217;s easy for an LLM to get Lisp or Scheme wrong because it loses track of\nhow many closing parentheses it has already emitted or is looking at.  Fixable\nwith future LLMs?  Sure, but also something that was hard for humans to get\nright too without tooling.</p>\n<h3>Flow Context But Explicit</h3>\n<p>Readers of this blog might know that I&#8217;m a huge believer in async locals and\nflow execution context — basically the ability to carry data through every\ninvocation that might only be needed many layers down the call chain.  Working\nat an observability company has really driven home the importance of this for\nme.</p>\n<p>The challenge is that anything that flows implicitly might not be configured.\nTake for instance the current time.  You might want to implicitly pass a timer\nto all functions.  But what if a timer is not configured and all of a sudden a\nnew dependency appears?  Passing all of it explicitly is tedious for both humans\nand agents and bad shortcuts will be made.</p>\n<p>One thing I&#8217;ve experimented with is having effect markers on functions that are\nadded through a code formatting step.  A function can declare that it needs the\ncurrent time or the database, but if it doesn&#8217;t mark this explicitly, it&#8217;s\nessentially a linting warning that auto-formatting fixes.  The LLM can start\nusing something like the current time in a function and any existing caller gets\nthe warning; formatting propagates the annotation.</p>\n<p>This is nice because when the LLM builds a test, it can precisely mock out\nthese side effects — it understands from the error messages what it has to\nsupply.</p>\n<p>For instance:</p>\n<div class="highlight"><pre><span></span><span class="k">fn</span><span class="w"> </span><span class="nf">issue</span><span class="p">(</span><span class="n">sub</span><span class="p">:</span><span class="w"> </span><span class="nc">UserId</span><span class="p">,</span><span class="w"> </span><span class="n">scopes</span><span class="p">:</span><span class="w"> </span><span class="p">[]</span><span class="n">Scope</span><span class="p">)</span><span class="w"> </span><span class="p">-&gt;</span><span class="w"> </span><span class="nc">Token</span>\n<span class="w">    </span><span class="n">needs</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">time</span><span class="p">,</span><span class="w"> </span><span class="n">rng</span><span class="w"> </span><span class="p">}</span>\n<span class="p">{</span>\n<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">Token</span><span class="p">{</span>\n<span class="w">        </span><span class="n">sub</span><span class="p">,</span>\n<span class="w">        </span><span class="n">exp</span><span class="p">:</span><span class="w"> </span><span class="nc">time</span><span class="p">.</span><span class="n">now</span><span class="p">().</span><span class="n">add</span><span class="p">(</span><span class="mi">24</span><span class="n">h</span><span class="p">),</span>\n<span class="w">        </span><span class="n">scopes</span><span class="p">,</span>\n<span class="w">    </span><span class="p">}</span>\n<span class="p">}</span>\n\n<span class="n">test</span><span class="w"> </span><span class="s">&quot;issue creates exp in the future&quot;</span><span class="w"> </span><span class="p">{</span>\n<span class="w">    </span><span class="n">using</span><span class="w"> </span><span class="n">time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">time</span><span class="p">.</span><span class="n">fixed</span><span class="p">(</span><span class="s">&quot;2026-02-06T23:00:00Z&quot;</span><span class="p">);</span>\n<span class="w">    </span><span class="n">using</span><span class="w"> </span><span class="n">rng</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">rng</span><span class="p">.</span><span class="n">deterministic</span><span class="p">(</span><span class="n">seed</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>\n\n<span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">issue</span><span class="p">(</span><span class="n">user</span><span class="p">(</span><span class="s">&quot;u1&quot;</span><span class="p">),</span><span class="w"> </span><span class="p">[</span><span class="s">&quot;read&quot;</span><span class="p">]);</span>\n<span class="w">    </span><span class="n">assert</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="n">exp</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">time</span><span class="p">.</span><span class="n">now</span><span class="p">());</span>\n<span class="p">}</span>\n</pre></div>\n<h3>Results over Exceptions</h3>\n<p>Agents struggle with exceptions, they are afraid of them.  I&#8217;m not sure to what\ndegree this is solvable with RL (Reinforcement Learning), but right now agents\nwill try to catch everything they can, log it, and do a pretty poor recovery.\nGiven how little information is actually available about error paths, that makes\nsense.  Checked exceptions are one approach, but they propagate all the way up\nthe call chain and don&#8217;t dramatically improve things.  Even if they end up as\nhints where a linter tracks which errors can fly by, there are still many call\nsites that need adjusting.  And like the auto-propagation proposed for context\ndata, it might not be the right solution.</p>\n<p>Maybe the right approach is to go more in on typed results, but that&#8217;s still\ntricky for composability without a type and object system that supports it.</p>\n<h3>Minimal Diffs and Line Reading</h3>\n<p>The general approach agents use today to read files into memory is line-based,\nwhich means they often pick chunks that span multi-line strings.  One easy way\nto see this fall apart: have an agent work on a 2000-line file that also\ncontains long embedded code strings — basically a code generator.  The agent\nwill sometimes edit within a multi-line string assuming it&#8217;s the real code when\nit&#8217;s actually just embedded code in a multi-line string.  For multi-line\nstrings, the only language I&#8217;m aware of with a good solution is Zig, but its\nprefix-based syntax is pretty foreign to most people.</p>\n<p>Reformatting also often causes constructs to move to different lines.  In many\nlanguages, trailing commas in lists are either not supported (JSON) or not\ncustomary.  If you want diff stability, you&#8217;d aim for a syntax that requires\nless reformatting and mostly avoids multi-line constructs.</p>\n<h3>Make It Greppable</h3>\n<p>What&#8217;s really nice about Go is that you mostly cannot import symbols from\nanother package into scope without every use being prefixed with the package\nname.  Eg: <code>context.Context</code> instead of <code>Context</code>.  There are escape hatches\n(import aliases and dot-imports), but they&#8217;re relatively rare and usually\nfrowned upon.</p>\n<p>That dramatically helps an agent understand what it&#8217;s looking at.  In general,\nmaking code findable through the most basic tools is great — it works with\nexternal files that aren&#8217;t indexed, and it means fewer false positives for\nlarge-scale automation driven by code generated on the fly (eg: <code>sed</code>, <code>perl</code>\ninvocations).</p>\n<h3>Local Reasoning</h3>\n<p>Much of what I&#8217;ve said boils down to: agents really like local reasoning.  They\nwant it to work in parts because they often work with just a few loaded files in\ncontext and don&#8217;t have much spatial awareness of the codebase.  They rely on\nexternal tooling like grep to find things, and anything that&#8217;s hard to grep or\nthat hides information elsewhere is tricky.</p>\n<h3>Dependency Aware Builds</h3>\n<p>What makes agents fail or succeed in many languages is just how good the build\ntools are.  Many languages make it very hard to determine what actually needs to\nrebuild or be retested because there are too many cross-references.  Go is\nreally good here: it forbids circular dependencies between packages (import\ncycles), packages have a clear layout, and test results are cached.</p>\n<h2>What Agents Hate</h2>\n<h3>Macros</h3>\n<p>Agents often struggle with macros.  It was already pretty clear that humans\nstruggle with macros too, but the argument for them was mostly that code\ngeneration was a good way to have less code to write.  Since that is less of a\nconcern now, we should aim for languages with less dependence on macros.</p>\n<p>There&#8217;s a separate question about generics and\n<a href="https://zig.guide/language-basics/comptime/">comptime</a>.  I think they fare\nsomewhat better because they mostly generate the same structure with different\nplaceholders and it&#8217;s much easier for an agent to understand that.</p>\n<h3>Re-Exports and Barrel Files</h3>\n<p>Related to greppability: agents often struggle to understand <a href="https://tkdodo.eu/blog/please-stop-using-barrel-files">barrel\nfiles</a> and they don&#8217;t\nlike them.  Not being able to quickly figure out where a class or function comes\nfrom leads to imports from the wrong place, or missing things entirely and\nwasting context by reading too many files.  A one-to-one mapping from where\nsomething is declared to where it&#8217;s imported from is great.</p>\n<p>And it does not have to be overly strict either.  Go kind of goes this way, but\nnot too extreme.  Any file within a directory can define a function, which isn&#8217;t\noptimal, but it&#8217;s quick enough to find and you don&#8217;t need to search too far.\nIt works because packages are forced to be small enough to find everything with\ngrep.</p>\n<p>The worst case is free re-exports all over the place that completely decouple\nthe implementation from any trivially reconstructable location on disk.  Or\nworse: aliasing.</p>\n<h3>Aliasing</h3>\n<p>Agents often hate it when aliases are involved.  In fact, you can get them to\neven complain about it in thinking blocks if you let them refactor something\nthat uses lots of aliases.  Ideally a language encourages good naming and\ndiscourages aliasing at import time as a result.</p>\n<h3>Flaky Tests and Dev Env Divergence</h3>\n<p>Nobody likes flaky tests, but agents even less so.  Ironic given how\nparticularly good agents are at creating flaky tests in the first place.  That&#8217;s\nbecause agents currently love to mock and most languages do not support mocking\nwell.  So many tests end up accidentally not being concurrency safe or depend on\ndevelopment environment state that then diverges in CI or production.</p>\n<p>Most programming languages and frameworks make it much easier to write flaky\ntests than non-flaky ones.  That&#8217;s because they encourage indeterminism\neverywhere.</p>\n<h3>Multiple Failure Conditions</h3>\n<p>In an ideal world the agent has one command, that lints and compiles and it\ntells the agent if all worked out fine.  Maybe another command to run all tests\nthat need running.  In practice most environments don&#8217;t work like this.  For\ninstance in TypeScript you can often run the code even <a href="https://lucumr.pocoo.org/2025/8/4/shitty-types/">though it fails\ntype checks</a>.  That can gaslight the agent.  Likewise\ndifferent bundler setups can cause one thing to succeed just for a slightly\ndifferent setup in CI to fail later.  The more uniform the tooling the better.</p>\n<p>Ideally it either runs or doesn&#8217;t and there is mechanical fixing for as many\nlinting failures as possible so that the agent does not have to do it by hand.</p>\n<h2>Will We See New Languages?</h2>\n<p>I think we will.  We are writing more software now than we ever have — more\nwebsites, more open source projects, more of everything.  Even if the ratio of\nnew languages stays the same, the absolute number will go up.  But I also truly\nbelieve that many more people will be willing to rethink the foundations of\nsoftware engineering and the languages we work with.  That&#8217;s because while for\nsome years it has felt you need to build a lot of infrastructure for a language\nto take off, now you can target a rather narrow use case: make sure the agent is\nhappy and extend from there to the human.</p>\n<p>I just hope we see two things.  First, some outsider art: people who haven&#8217;t\nbuilt languages before trying their hand at it and showing us new things.\nSecond, a much more deliberate effort to document what works and what doesn&#8217;t\nfrom first principles.  We have actually learned a lot about what makes good\nlanguages and how to scale software engineering to large teams.  Yet,  finding\nit written down, as a consumable overview of good and bad language design, is\nvery hard to come by.  Too much of it has been shaped by opinion on rather\npointless things instead of hard facts.</p>\n<p>Now though, we are slowly getting to the point where facts matter more, because\nyou can actually measure what works by seeing how well agents perform with it.\nNo human wants to be subject to surveys, but <a href="https://lucumr.pocoo.org/2025/6/17/measuring/">agents don&#8217;t\ncare</a>.  We can see how successful they are and where they\nare struggling.</p>'}

---

*抓取时间: 2026-02-09 18:02:18*
