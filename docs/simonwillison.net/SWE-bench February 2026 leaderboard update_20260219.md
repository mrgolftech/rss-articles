# SWE-bench February 2026 leaderboard update

**来源:** [simonwillison.net](https://simonwillison.net)
**发布时间:** 2026-02-19T04:48:47+00:00
**链接:** https://simonwillison.net/2026/Feb/19/swe-bench/#atom-everything

---

<p><strong><a href="https://www.swebench.com/">SWE-bench February 2026 leaderboard update</a></strong></p>
SWE-bench is one of the benchmarks that the labs love to list in their model releases. The official leaderboard is infrequently updated but they just did a full run of it against the current generation of models, which is notable because it's always good to see benchmark results like this that <em>weren't</em> self-reported by the labs.</p>
<p>The fresh results are for their "Bash Only" benchmark, which runs their <a href="https://github.com/SWE-agent/mini-swe-agent">mini-swe-bench</a> agent (~9,000 lines of Python, <a href="https://github.com/SWE-agent/mini-swe-agent/blob/v2.2.1/src/minisweagent/config/benchmarks/swebench.yaml">here are the prompts</a> they use) against the <a href="https://huggingface.co/datasets/princeton-nlp/SWE-bench">SWE-bench</a> dataset of coding problems - 2,294 real-world examples pulled from 12 open source repos: <a href="https://github.com/django/django">django/django</a> (850), <a href="https://github.com/sympy/sympy">sympy/sympy</a> (386), <a href="https://github.com/scikit-learn/scikit-learn">scikit-learn/scikit-learn</a> (229), <a href="https://github.com/sphinx-doc/sphinx">sphinx-doc/sphinx</a> (187), <a href="https://github.com/matplotlib/matplotlib">matplotlib/matplotlib</a> (184), <a href="https://github.com/pytest-dev/pytest">pytest-dev/pytest</a> (119), <a href="https://github.com/pydata/xarray">pydata/xarray</a> (110), <a href="https://github.com/astropy/astropy">astropy/astropy</a> (95), <a href="https://github.com/pylint-dev/pylint">pylint-dev/pylint</a> (57), <a href="https://github.com/psf/requests">psf/requests</a> (44), <a href="https://github.com/mwaskom/seaborn">mwaskom/seaborn</a> (22), <a href="https://github.com/pallets/flask">pallets/flask</a> (11).</p>
<p><strong>Correction</strong>: <em>The Bash only benchmark runs against SWE-bench Verified, not original SWE-bench. Verified is a manually curated subset of 500 samples <a href="https://openai.com/index/introducing-swe-bench-verified/">described here</a>, funded by OpenAI. Here's <a href="https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified">SWE-bench Verified</a> on Hugging Face - since it's just 2.1MB of Parquet it's easy to browse <a href="https://lite.datasette.io/?parquet=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fprinceton-nlp%2FSWE-bench_Verified%2Fresolve%2Fmain%2Fdata%2Ftest-00000-of-00001.parquet#/data/test-00000-of-00001?_facet=repo">using Datasette Lite</a>, which cuts those numbers down to django/django (231), sympy/sympy (75), sphinx-doc/sphinx (44), matplotlib/matplotlib (34), scikit-learn/scikit-learn (32), astropy/astropy (22), pydata/xarray (22), pytest-dev/pytest (19), pylint-dev/pylint (10), psf/requests (8), mwaskom/seaborn (2), pallets/flask (1)</em>.</p>
<p>Here's how the top ten models performed:</p>
<p><img alt="Bar chart showing &quot;% Resolved&quot; by &quot;Model&quot;. Bars in descending order: Claude 4.5 Opus (high reasoning) 76.8%, Gemini 3 Flash (high reasoning) 75.8%, MiniMax M2.5 (high reasoning) 75.8%, Claude Opus 4.6 75.6%, GLM-5 (high reasoning) 72.8%, GPT-5.2 (high reasoning) 72.8%, Claude 4.5 Sonnet (high reasoning) 72.8%, Kimi K2.5 (high reasoning) 71.4%, DeepSeek V3.2 (high reasoning) 70.8%, Claude 4.5 Haiku (high reasoning) 70.0%, and a partially visible final bar at 66.6%." src="https://static.simonwillison.net/static/2026/swbench-feb-2026.jpg" /></p>
<p>It's interesting to see Claude Opus 4.5 beat Opus 4.6, though only by about a percentage point. 4.5 Opus is top, then Gemini 3 Flash, then MiniMax M2.5 - a 229B model released <a href="https://www.minimax.io/news/minimax-m25">last week</a> by Chinese lab MiniMax. GLM-5, Kimi K2.5 and DeepSeek V3.2 are three more Chinese models that make the top ten as well.</p>
<p>OpenAI's GPT-5.2 is their highest performing model at position 6, but it's worth noting that their best coding model, GPT-5.3-Codex, is not represented - maybe because it's not yet available in the OpenAI API.</p>
<p>This benchmark uses the same system prompt for every model, which is important for a fair comparison but does mean that the quality of the different harnesses or optimized prompts is not being measured here.</p>
<p>The chart above is a screenshot from the SWE-bench website, but their charts don't include the actual percentage values visible on the bars. I successfully used Claude for Chrome to add these - <a href="https://claude.ai/share/81a0c519-c727-4caa-b0d4-0d866375d0da">transcript here</a>. My prompt sequence included:</p>
<blockquote>
<p>Use claude in chrome to open https://www.swebench.com/</p>
<p>Click on "Compare results" and then select "Select top 10"</p>
<p>See those bar charts? I want them to display the percentage on each bar so I can take a better screenshot, modify the page like that</p>
</blockquote>
<p>I'm impressed at how well this worked - Claude injected custom JavaScript into the page to draw additional labels on top of the existing chart.</p>
<p><img alt="Screenshot of a Claude AI conversation showing browser automation. A thinking step reads &quot;Pivoted strategy to avoid recursion issues with chart labeling &gt;&quot; followed by the message &quot;Good, the chart is back. Now let me carefully add the labels using an inline plugin on the chart instance to avoid the recursion issue.&quot; A collapsed &quot;Browser_evaluate&quot; section shows a browser_evaluate tool call with JavaScript code using Chart.js canvas context to draw percentage labels on bars: meta.data.forEach((bar, index) =&gt; { const value = dataset.data[index]; if (value !== undefined &amp;&amp; value !== null) { ctx.save(); ctx.textAlign = 'center'; ctx.textBaseline = 'bottom'; ctx.fillStyle = '#333'; ctx.font = 'bold 12px sans-serif'; ctx.fillText(value.toFixed(1) + '%', bar.x, bar.y - 5); A pending step reads &quot;Let me take a screenshot to see if it worked.&quot; followed by a completed &quot;Done&quot; step, and the message &quot;Let me take a screenshot to check the result.&quot;" src="https://static.simonwillison.net/static/2026/claude-chrome-draw-on-chart.jpg" /></p>
<p><strong>Update</strong>: If you look at the transcript Claude claims to have switched to Playwright, which is confusing because I didn't think I had that configured.

    <p><small></small>Via <a href="https://twitter.com/KLieret/status/2024176335782826336">@KLieret</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/benchmarks">benchmarks</a>, <a href="https://simonwillison.net/tags/django">django</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/ai-in-china">ai-in-china</a>, <a href="https://simonwillison.net/tags/minimax">minimax</a></p>

---

*抓取时间: 2026-02-21 00:09:53*
