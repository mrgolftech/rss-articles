# Structured Context Engineering for File-Native Agentic Systems

**来源:** [simonwillison.net](https://simonwillison.net)
**发布时间:** 2026-02-09T23:56:51+00:00
**链接:** https://simonwillison.net/2026/Feb/9/structured-context-engineering-for-file-native-agentic-systems/#atom-everything

---

<p><strong><a href="https://arxiv.org/abs/2602.05447">Structured Context Engineering for File-Native Agentic Systems</a></strong></p>
New paper by Damon McMillan exploring challenging LLM context tasks involving large SQL schemas (up to 10,000 tables) across different models and file formats:</p>
<blockquote>
<p>Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.</p>
</blockquote>
<p>Unsurprisingly, the biggest impact was the models themselves - with frontier models (Opus 4.5, GPT-5.2, Gemini 2.5 Pro) beating the leading open source models (DeepSeek V3.2, Kimi K2, Llama 4).</p>
<p>Those frontier models benefited from filesystem based context retrieval, but the open source models had much less convincing results with those, which reinforces my feeling that the filesystem coding agent loops aren't handled as well by open weight models just yet. The <a href="https://www.tbench.ai/leaderboard/terminal-bench/2.0">Terminal Bench 2.0</a> leaderboard is still dominated by Anthropic, OpenAI and Gemini.</p>
<p>The "grep tax" result against <a href="https://github.com/toon-format/toon">TOON</a> was an interesting detail. TOON is meant to represent structured data in as few tokens as possible, but it turns out the model's unfamiliarity with that format led to them spending significantly more tokens over multiple iterations trying to figure it out:</p>
<p><img alt="Screenshot of a figure from a research paper. Introductory text reads: &quot;As schema size increased, TOON showed dramatically increased token consumption for Claude models despite being ~25% smaller in file size. Scale experiments used Claude models only.&quot; Below is &quot;Figure 7: The 'Grep Tax' - TOON Token Overhead at Scale&quot;, a bar chart with a logarithmic y-axis labeled &quot;Tokens&quot; comparing YAML (teal) and TOON (purple) at two schema sizes: S5 (500 tables) and S9 (10,000 tables). At S5, TOON is +138% more tokens than YAML (~1,100 vs ~450). At S9, TOON is +740% more tokens (~50,000 vs ~7,000). Below the chart, explanatory text reads: &quot;The 'grep tax' emerged as schema size scaled. At S5 (500 tables), TOON consumed 138% more tokens than YAML; at S9 (10,000 tables), this grew to 740%. Root cause: models lacked familiarity with TOON's syntax and could not construct effective refinement patterns.&quot;" src="https://static.simonwillison.net/static/2026/grep-tax.jpg" />

    <p><small></small>Via <a href="https://twitter.com/omarsar0/status/2020150077637997013">@omarsar0</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/prompt-engineering">prompt-engineering</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/paper-review">paper-review</a>, <a href="https://simonwillison.net/tags/context-engineering">context-engineering</a></p>

---

*抓取时间: 2026-02-10 12:02:45*
