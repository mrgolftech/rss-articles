# Rewriting pycparser with the help of an LLM

**来源:** [eli.thegreenplace.net](https://eli.thegreenplace.net)
**发布时间:** 2026-02-04T19:35:00-08:00
**链接:** https://eli.thegreenplace.net/2026/rewriting-pycparser-with-the-help-of-an-llm/

---

{'type': 'text/html', 'language': None, 'base': 'https://eli.thegreenplace.net/feeds/all.atom.xml', 'value': '<p><a class="reference external" href="https://github.com/eliben/pycparser">pycparser</a> is my most widely used open\nsource project (with ~20M daily downloads from PyPI <a class="footnote-reference" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-1" id="footnote-reference-1">[1]</a>). It\'s a pure-Python\nparser for the C programming language, producing ASTs inspired by <a class="reference external" href="https://docs.python.org/3/library/ast.html">Python\'s\nown</a>. Until very recently, it\'s\nbeen using <a class="reference external" href="https://www.dabeaz.com/ply/ply.html">PLY: Python Lex-Yacc</a> for\nthe core parsing.</p>\n<p>In this post, I\'ll describe how I collaborated with an LLM coding agent (Codex)\nto help me rewrite pycparser to use a hand-written recursive-descent parser and\nremove the dependency on PLY. This has been an interesting experience and the\npost contains lots of information and is therefore quite long; if you\'re just\ninterested in the final result, check out the latest code of pycparser - the\n<tt class="docutils literal">main</tt> branch already has the new implementation.</p>\n<img alt="meme picture saying &quot;can\'t come to bed because my AI agent produced something slightly wrong&quot;" class="align-center" src="https://eli.thegreenplace.net/images/2026/cantcometobed.png" />\n<div class="section" id="the-issues-with-the-existing-parser-implementation">\n<h2>The issues with the existing parser implementation</h2>\n<p>While pycparser has been working well overall, there were a number of nagging\nissues that persisted over years.</p>\n<div class="section" id="parsing-strategy-yacc-vs-hand-written-recursive-descent">\n<h3>Parsing strategy: YACC vs. hand-written recursive descent</h3>\n<p>I began working on pycparser in 2008, and back then using a YACC-based approach\nfor parsing a whole language like C seemed like a no-brainer to me. Isn\'t this\nwhat everyone does when writing a serious parser? Besides, the K&amp;R2 book\nfamously carries the entire grammar of the C99 language in an appendix - so it\nseemed like a simple matter of translating that to PLY-yacc syntax.</p>\n<p>And indeed, it wasn\'t <em>too</em> hard, though there definitely were some complications\nin building the ASTs for declarations (C\'s <a class="reference external" href="https://eli.thegreenplace.net/2008/10/18/implementing-cdecl-with-pycparser">gnarliest part</a>).</p>\n<p>Shortly after completing pycparser, I got more and more interested in compilation\nand started learning about the different kinds of parsers more seriously. Over\ntime, I grew convinced that <a class="reference external" href="https://eli.thegreenplace.net/tag/recursive-descent-parsing">recursive descent</a> is the way to\ngo - producing parsers that are easier to understand and maintain (and are often\nfaster!).</p>\n<p>It all ties in to the <a class="reference external" href="https://eli.thegreenplace.net/2017/benefits-of-dependencies-in-software-projects-as-a-function-of-effort/">benefits of dependencies in software projects as a\nfunction of effort</a>.\nUsing parser generators is a heavy <em>conceptual</em> dependency: it\'s really nice\nwhen you have to churn out many parsers for small languages. But when you have\nto maintain a single, very complex parser, as part of a large project - the\nbenefits quickly dissipate and you\'re left with a substantial dependency that\nyou constantly grapple with.</p>\n</div>\n<div class="section" id="the-other-issue-with-dependencies">\n<h3>The other issue with dependencies</h3>\n<p>And then there are the usual problems with dependencies; dependencies get\nabandoned, and they may also develop security issues. Sometimes, both of these\nbecome true.</p>\n<p>Many years ago, pycparser forked and started vendoring its own version of PLY.\nThis was part of transitioning pycparser to a dual Python 2/3 code base when PLY\nwas slower to adapt. I believe this was the right decision, since PLY &quot;just\nworked&quot; and I didn\'t have to deal with active (and very tedious in the Python\necosystem, where packaging tools are replaced faster than dirty socks)\ndependency management.</p>\n<p>A couple of weeks ago <a class="reference external" href="https://github.com/eliben/pycparser/issues/588">this issue</a>\nwas opened for pycparser. It turns out the some old PLY code triggers security\nchecks used by some Linux distributions; while this code was fixed in a later\ncommit of PLY, PLY itself was apparently abandoned and archived in late 2025.\nAnd guess what? That happened in the middle of a large rewrite of the package,\nso re-vendoring the pre-archiving commit seemed like a risky proposition.</p>\n<p>On the issue it was suggested that &quot;hopefully the dependent packages move on to\na non-abandoned parser or implement their own&quot;; I originally laughed this idea\noff, but then it got me thinking... which is what this post is all about.</p>\n</div>\n<div class="section" id="growing-complexity-of-parsing-a-messy-language">\n<h3>Growing complexity of parsing a messy language</h3>\n<p>The original K&amp;R2 grammar for C99 had - famously - a single shift-reduce\nconflict having to do with dangling <tt class="docutils literal">else</tt>s belonging to the most recent\n<tt class="docutils literal">if</tt> statement. And indeed, other than the famous <a class="reference external" href="https://en.wikipedia.org/wiki/Lexer_hack">lexer hack</a>\nused to deal with <a class="reference external" href="https://eli.thegreenplace.net/2011/05/02/the-context-sensitivity-of-cs-grammar-revisited">C\'s type name / ID ambiguity</a>,\npycparser only had this single shift-reduce conflict.</p>\n<p>But things got more complicated. Over the years, features were added that\nweren\'t strictly in the standard but were supported by all the industrial\ncompilers. The more advanced C11 and C23 standards weren\'t beholden to the\npromises of conflict-free YACC parsing (since almost no industrial-strength\ncompilers use YACC at this point), so all caution went out of the window.</p>\n<p>The latest (PLY-based) release of pycparser has many reduce-reduce conflicts\n<a class="footnote-reference" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-2" id="footnote-reference-2">[2]</a>; these are a severe maintenance hazard because it means the parsing rules\nessentially have to be tie-broken by order of appearance in the code. This is\nvery brittle; pycparser has only managed to maintain its stability and quality\nthrough its comprehensive test suite. Over time, it became harder and harder to\nextend, because YACC parsing rules have all kinds of spooky-action-at-a-distance\neffects. The straw that broke the camel\'s back was <a class="reference external" href="https://github.com/eliben/pycparser/pull/590">this PR</a> which again proposed to\nincrease the number of reduce-reduce conflicts <a class="footnote-reference" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-3" id="footnote-reference-3">[3]</a>.</p>\n<p>This - again - prompted me to think &quot;what if I just dump YACC and switch to\na hand-written recursive descent parser&quot;, and here we are.</p>\n</div>\n</div>\n<div class="section" id="the-mental-roadblock">\n<h2>The mental roadblock</h2>\n<p>None of the challenges described above are new; I\'ve been pondering them for\nmany years now, and yet biting the bullet and rewriting the parser didn\'t feel\nlike something I\'d like to get into. By my private estimates it\'d take at least\na week of deep heads-down work to port the gritty 2000 lines of YACC grammar\nrules to a recursive descent parser <a class="footnote-reference" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-4" id="footnote-reference-4">[4]</a>. Moreover, it wouldn\'t be a\nparticularly <em>fun</em> project either - I didn\'t feel like I\'d learn much new and\nmy interests have shifted away from this project. In short, the <a class="reference external" href="https://en.wikipedia.org/wiki/Potential_well">Potential well</a> was just too deep.</p>\n</div>\n<div class="section" id="why-would-this-even-work-tests">\n<h2>Why would this even work? Tests</h2>\n<p>I\'ve definitely noticed the improvement in capabilities of LLM coding\nagents in the past few months, and many reputable people online rave about using\nthem for increasingly larger projects. That said, would an LLM agent really be\nable to accomplish such a complex project on its own? This isn\'t just a toy,\nit\'s thousands of lines of dense parsing code.</p>\n<p>What gave me hope is the concept of <a class="reference external" href="https://simonwillison.net/2025/Dec/31/the-year-in-llms/#the-year-of-conformance-suites">conformance suites mentioned by\nSimon Willison</a>.\nAgents seem to do well when there\'s a very clear and rigid\ngoal function - such as a large, high-coverage conformance test suite.</p>\n<p>And pycparser has an <a class="reference external" href="https://github.com/eliben/pycparser/blob/main/tests/test_c_parser.py">very extensive one</a>.\nOver 2500 lines of test code parsing various C snippets to ASTs with expected\nresults, grown over a decade and a half of real issues and bugs reported by\nusers.</p>\n<p>I figured the LLM can either succeed or fail and throw its hands up in despair,\nbut it\'s quite unlikely to produce a <em>wrong</em> port that would still pass all\nthe tests. So I set it to run.</p>\n</div>\n<div class="section" id="the-initial-port">\n<h2>The initial port</h2>\n<p>I fired up Codex in pycparser\'s repository, and wrote this prompt just to\nmake sure it understands me and can run the tests:</p>\n<div class="highlight"><pre><span></span>This repository implements a Python parser producing an AST. It has a very\nextensive test suite; can you first verify you successfully run all the tests?\n(with python3 -m unittest discover, from the root dir)\n</pre></div>\n<p>Codex figured it out (I gave it the exact command, after all!); my next\nprompt was the real thing <a class="footnote-reference" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-5" id="footnote-reference-5">[5]</a>:</p>\n<div class="highlight"><pre><span></span>pycparser uses the PLY library for its lexer and parser, and its own AST\npackage for the ASTs created by the parser. I want to explore replacing the\nParser component (leaving the Lexer alone for now) with a hand written\nrecursive descent parser that will pass all the same tests. Can you try that?\n</pre></div>\n<p>Here Codex went to work and churned <em>for over an hour</em>. Having never\nobserved an agent work for nearly this long, I kind of assumed it went off\nthe rails and will fail sooner or later. So I was rather surprised and skeptical\nwhen it eventually came back with:</p>\n<div class="highlight"><pre><span></span>Replaced the PLY parse path with a handwritten recursive‑descent parser while\nkeeping the existing lexer and AST helpers intact; CParser now points to the\nnew RD implementation, with the old PLY version preserved as PLYCParser in\npycparser/c_parser.py. The RD parser includes a token stream, declarator/type\nparsing, expression precedence handling, pragmas, and statement parsing to\nmatch the previous AST output and coordinate behavior.\n</pre></div>\n<p>It took me a while to poke around the code and run it until I was convinced - it\nhad actually done it! It wrote a new recursive descent parser with only\nancillary dependencies on PLY, and that parser passed the test suite. After a\nfew more prompts, we\'ve removed the ancillary dependencies and made the\nstructure clearer. I hadn\'t looked too deeply into code quality at this point,\nbut at least on the functional level - it succeeded. This was very impressive!</p>\n</div>\n<div class="section" id="a-quick-note-on-reviews-and-branches">\n<h2>A quick note on reviews and branches</h2>\n<p>A change like the one described above is impossible to code-review as one PR in\nany meaningful way; so I used a different strategy. Before embarking on this\npath, I created a new branch and once Codex finished the initial rewrite, I\ncommitted this change, knowing that I will review it in detail, piece-by-piece\nlater on.</p>\n<p>Even though coding agents have their own notion of history and can &quot;revert&quot;\ncertain changes, I felt much safer relying on Git. In the worst case if all of\nthis goes south, I can nuke the branch and it\'s as if nothing ever happened.\nI was determined to only merge this branch onto <tt class="docutils literal">main</tt> once I was fully\nsatisfied with the code. In what follows, I had to <tt class="docutils literal">git reset</tt> several times\nwhen I didn\'t like the direction in which Codex was going. In hindsight, doing\nthis work in a branch was absolutely the right choice.</p>\n</div>\n<div class="section" id="the-long-tail-of-goofs">\n<h2>The long tail of goofs</h2>\n<p>Once I\'ve sufficiently convinced myself that the new parser is actually working,\nI used Codex to similarly rewrite the lexer and get rid of the PLY dependency\nentirely, deleting it from the repository. Then, I started looking more deeply\ninto code quality - reading the code created by Codex and trying to wrap my head\naround it.</p>\n<p>And - oh my - this was quite the journey. Much has been written about the code\nproduced by agents, and much of it seems to be true. Maybe it\'s a setting I\'m\nmissing (I\'m not using my own custom <tt class="docutils literal">AGENTS.md</tt> yet, for instance), but\nCodex seems to be that eager programmer that wants to get from A to B whatever\nthe cost. Readability, minimalism and code clarity are very much secondary\ngoals.</p>\n<p>Using <tt class="docutils literal"><span class="pre">raise...except</span></tt> for control flow? Yep. Abusing Python\'s weak typing\n(like having <tt class="docutils literal">None</tt>, <tt class="docutils literal">false</tt> and other values all mean different things\nfor a given variable)? For sure. Spreading the logic of a complex function\nall over the place instead of putting all the key parts in a single switch\nstatement? You bet.</p>\n<p>Moreover, the agent is hilariously <em>lazy</em>. More than once I had to convince it\nto do something it initially said is impossible, and even insisted again in\nfollow-up messages. The anthropomorphization here is mildly concerning, to be\nhonest. I could never imagine I would be writing something like the following to\na computer, and yet - here we are: &quot;Remember how we moved X to Y before? You\ncan do it again for Z, definitely. Just try&quot;.</p>\n<p>My process was to see how I can instruct Codex to fix things, and intervene\nmyself (by rewriting code) as little as possible. I\'ve <em>mostly</em> succeeded in\nthis, and did maybe 20% of the work myself.</p>\n<p>My branch grew <em>dozens</em> of commits, falling into roughly these categories:</p>\n<ol class="arabic simple">\n<li>The code in X is too complex; why can\'t we do Y instead?</li>\n<li>The use of X is needlessly convoluted; change Y to Z, and T to V in all\ninstances.</li>\n<li>The code in X is unclear; please add a detailed comment - with examples - to\nexplain what it does.</li>\n</ol>\n<p>Interestingly, after doing (3), the agent was often more effective in giving\nthe code a &quot;fresh look&quot; and succeeding in either (1) or (2).</p>\n</div>\n<div class="section" id="the-end-result">\n<h2>The end result</h2>\n<p>Eventually, after many hours spent in this process, I was reasonably pleased\nwith the code. It\'s far from perfect, of course, but taking the essential\ncomplexities into account, it\'s something I could see myself maintaining (with\nor without the help of an agent). I\'m sure I\'ll find more ways to improve it\nin the future, but I have a reasonable degree of confidence that this will be\ndoable.</p>\n<p>It passes all the tests, so I\'ve been able to release a new version (3.00)\nwithout major issues so far. The only issue I\'ve discovered is that some of\nCFFI\'s tests are overly precise about the phrasing of errors reported by\npycparser; this was <a class="reference external" href="https://github.com/python-cffi/cffi/pull/224">an easy fix</a>.</p>\n<p>The new parser is also faster, by about 30% based on my benchmarks! This is\ntypical of recursive descent when compared with YACC-generated parsers, in my\nexperience. After reviewing the initial rewrite of the lexer, I\'ve spent a while\ninstructing Codex on how to make it faster, and it worked reasonably well.</p>\n</div>\n<div class="section" id="followup-static-typing">\n<h2>Followup - static typing</h2>\n<p>While working on this, it became quite obvious that static typing would make the\nprocess easier. LLM coding agents really benefit from closed loops with strict\nguardrails (e.g. a test suite to pass), and type-annotations act as such.\nFor example, had pycparser already been type annotated, Codex would probably not\nhave overloaded values to multiple types (like <tt class="docutils literal">None</tt> vs. <tt class="docutils literal">False</tt> vs.\nothers).</p>\n<p>In a followup, I asked Codex to type-annotate pycparser (running checks using\n<tt class="docutils literal">ty</tt>), and this was also a back-and-forth because the process exposed some\nissues that needed to be refactored. Time will tell, but hopefully it will make\nfurther changes in the project simpler for the agent.</p>\n<p>Based on this experience, I\'d bet that coding agents will be somewhat more\neffective in strongly typed languages like Go, TypeScript and especially Rust.</p>\n</div>\n<div class="section" id="conclusions">\n<h2>Conclusions</h2>\n<p>Overall, this project has been a really good experience, and I\'m impressed with\nwhat modern LLM coding agents can do! While there\'s no reason to expect that\nprogress in this domain will stop, even if it does - these are already very\nuseful tools that can significantly improve programmer productivity.</p>\n<p>Could I have done this myself, without an agent\'s help? Sure. But it would have\ntaken me <em>much</em> longer, assuming that I could even muster the will and\nconcentration to engage in this project. I estimate it would take me at least\na week of full-time work (so 30-40 hours) spread over who knows how long to\naccomplish. With Codex, I put in an order of magnitude less work into this\n(around 4-5 hours, I\'d estimate) and I\'m happy with the result.</p>\n<p>It was also <em>fun</em>. At least in one sense, my professional life can be described\nas the pursuit of focus, deep work and <em>flow</em>. It\'s not easy for me to get into\nthis state, but when I do I\'m highly productive and find it very enjoyable.\nAgents really help me here. When I know I need to write some code and it\'s\nhard to get started, asking an agent to write a prototype is a great catalyst\nfor my motivation. Hence the meme at the beginning of the post.</p>\n<div class="section" id="does-code-quality-even-matter">\n<h3>Does code quality even matter?</h3>\n<p>One can\'t avoid a nagging question - does the quality of the code produced\nby agents even matter? Clearly, the agents themselves can understand it (if not\ntoday\'s agent, then at least next year\'s). Why worry about future\nmaintainability if the agent can maintain it? In other words, does it make sense\nto just go full vibe-coding?</p>\n<p>This is a fair question, and one I don\'t have an answer to. Right now, for\nprojects I maintain and <em>stand behind</em>, it seems obvious to me that the code\nshould be fully understandable and accepted by me, and the agent is just a tool\nhelping me get to that state more efficiently. It\'s hard to say what the future\nholds here; it\'s going to interesting, for sure.</p>\n<hr class="docutils" />\n<table class="docutils footnote" frame="void" id="footnote-1" rules="none">\n<colgroup><col class="label" /><col /></colgroup>\n<tbody valign="top">\n<tr><td class="label"><a class="fn-backref" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-reference-1">[1]</a></td><td>pycparser has a fair number of <a class="reference external" href="https://deps.dev/pypi/pycparser/3.0.0/dependents">direct dependents</a>,\nbut the majority of downloads comes through <a class="reference external" href="https://github.com/python-cffi/cffi">CFFI</a>,\nwhich itself is a major building block for much of the Python ecosystem.</td></tr>\n</tbody>\n</table>\n<table class="docutils footnote" frame="void" id="footnote-2" rules="none">\n<colgroup><col class="label" /><col /></colgroup>\n<tbody valign="top">\n<tr><td class="label"><a class="fn-backref" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-reference-2">[2]</a></td><td>The table-building report says 177, but that\'s certainly an\nover-dramatization because it\'s common for a single conflict to\nmanifest in several ways.</td></tr>\n</tbody>\n</table>\n<table class="docutils footnote" frame="void" id="footnote-3" rules="none">\n<colgroup><col class="label" /><col /></colgroup>\n<tbody valign="top">\n<tr><td class="label"><a class="fn-backref" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-reference-3">[3]</a></td><td>It didn\'t help the PR\'s case that it was almost certainly vibe coded.</td></tr>\n</tbody>\n</table>\n<table class="docutils footnote" frame="void" id="footnote-4" rules="none">\n<colgroup><col class="label" /><col /></colgroup>\n<tbody valign="top">\n<tr><td class="label"><a class="fn-backref" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-reference-4">[4]</a></td><td><p class="first">There was also the lexer to consider, but this seemed like a much\nsimpler job. My impression is that in the early days of computing,\n<tt class="docutils literal">lex</tt> gained prominence because of strong regexp support which wasn\'t\nvery common yet. These days, with excellent regexp libraries\nexisting for pretty much every language, the added value of <tt class="docutils literal">lex</tt> over\na <a class="reference external" href="https://eli.thegreenplace.net/2013/06/25/regex-based-lexical-analysis-in-python-and-javascript">custom regexp-based lexer</a>\nisn\'t very high.</p>\n<p class="last">That said, it wouldn\'t make much sense to embark on a journey to rewrite\n<em>just</em> the lexer; the dependency on PLY would still remain, and besides,\nPLY\'s lexer and parser are designed to work well together. So it wouldn\'t\nhelp me much without tackling the parser beast.</p>\n</td></tr>\n</tbody>\n</table>\n<table class="docutils footnote" frame="void" id="footnote-5" rules="none">\n<colgroup><col class="label" /><col /></colgroup>\n<tbody valign="top">\n<tr><td class="label"><a class="fn-backref" href="https://eli.thegreenplace.net/feeds/all.atom.xml#footnote-reference-5">[5]</a></td><td>I\'ve decided to ask it to the port the parser first, leaving the lexer\nalone. This was to split the work into reasonable chunks. Besides, I\nfigured that the parser is the hard job anyway - if it succeeds in that,\nthe lexer should be easy. That assumption turned out to be correct.</td></tr>\n</tbody>\n</table>\n</div>\n</div>'}

---

*抓取时间: 2026-02-06 16:29:16*
