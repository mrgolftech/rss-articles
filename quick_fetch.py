#!/usr/bin/env python3
import os
import re
import time
import random
import json
import subprocess
from pathlib import Path
import xml.etree.ElementTree as ET
import html

def fetch_rss_with_curl(url):
    """Fetch RSS feed using curl with proper headers."""
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    ]
    
    user_agent = random.choice(user_agents)
    cmd = [
        'curl', '-s', '-L',
        '-A', user_agent,
        '--connect-timeout', '10',
        '--max-time', '20',
        url
    ]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=25)
        if result.returncode == 0 and result.stdout:
            return result.stdout
        return None
    except:
        return None

def parse_rss_articles(xml_content, max_items=3):
    """Parse RSS and return recent articles."""
    try:
        root = ET.fromstring(xml_content)
        articles = []
        
        # Handle both RSS and Atom formats
        items = root.findall('.//item') or root.findall('.//{http://www.w3.org/2005/Atom}entry')
        
        for item in items[:max_items]:
            title = ''
            link = ''
            desc = ''
            pub_date = ''
            
            # Get title
            title_elem = item.find('title')
            if title_elem is not None and title_elem.text:
                title = html.unescape(title_elem.text)
            else:
                title_elem = item.find('{http://www.w3.org/2005/Atom}title')
                if title_elem is not None and title_elem.text:
                    title = html.unescape(title_elem.text)
            
            # Get link
            link_elem = item.find('link')
            if link_elem is not None and link_elem.text:
                link = link_elem.text
            else:
                link_elem = item.find('{http://www.w3.org/2005/Atom}link')
                if link_elem is not None:
                    link = link_elem.get('href', '')
            
            # Get description/content
            desc_elem = item.find('description')
            if desc_elem is not None and desc_elem.text:
                desc = html.unescape(desc_elem.text)
            else:
                desc_elem = item.find('{http://www.w3.org/2005/Atom}content')
                if desc_elem is not None and desc_elem.text:
                    desc = html.unescape(desc_elem.text)
                else:
                    desc_elem = item.find('{http://www.w3.org/2005/Atom}summary')
                    if desc_elem is not None and desc_elem.text:
                        desc = html.unescape(desc_elem.text)
            
            # Get publication date
            date_elem = item.find('pubDate')
            if date_elem is not None and date_elem.text:
                pub_date = date_elem.text
            else:
                date_elem = item.find('{http://www.w3.org/2005/Atom}published')
                if date_elem is not None and date_elem.text:
                    pub_date = date_elem.text
            
            if title and link:
                # Clean up description
                desc = re.sub(r'<[^>]+>', ' ', desc)
                desc = ' '.join(desc.split())
                if len(desc) > 150:
                    desc = desc[:150] + '...'
                elif not desc:
                    desc = 'æš‚æ— æ‘˜è¦'
                
                articles.append({
                    'title': title,
                    'link': link,
                    'description': desc,
                    'date': pub_date
                })
        
        return articles
    except Exception as e:
        print(f"è§£æé”™è¯¯: {e}")
        return []

def main():
    # Popular tech blogs to check
    blogs = [
        ('simonwillison.net', 'https://simonwillison.net/atom/everything/', 'Python/Djangoä¸“å®¶'),
        ('paulgraham.com', 'http://www.aaronsw.com/2002/feeds/pgessays.rss', 'YCåˆ›å§‹äºº'),
        ('antirez.com', 'http://antirez.com/rss', 'Redisä½œè€…'),
        ('jeffgeerling.com', 'https://www.jeffgeerling.com/blog.xml', 'ç¡¬ä»¶å’ŒåµŒå…¥å¼ä¸“å®¶'),
        ('overreacted.io', 'https://overreacted.io/rss.xml', 'Reactæ ¸å¿ƒå¼€å‘è€…'),
        ('krebsonsecurity.com', 'https://krebsonsecurity.com/feed/', 'ç½‘ç»œå®‰å…¨ä¸“å®¶'),
        ('fabiensanglard.net', 'https://fabiensanglard.net/rss.xml', 'å›¾å½¢ç¼–ç¨‹ä¸“å®¶'),
        ('gwern.net', 'https://gwern.substack.com/feed', 'æ·±åº¦æ€è€ƒè€…'),
    ]
    
    all_articles = []
    
    print("ğŸš€ å¼€å§‹æŠ“å–RSS Feeds...")
    print("=" * 50)
    
    for name, rss_url, description in blogs:
        print(f"\nğŸ“¡ æŠ“å–: {name}")
        print(f"   {description}")
        
        xml_content = fetch_rss_with_curl(rss_url)
        if not xml_content:
            print(f"   âŒ æ— æ³•è·å–RSS")
            continue
        
        articles = parse_rss_articles(xml_content)
        if not articles:
            print(f"   âŒ æ— æ–‡ç« ")
            continue
        
        print(f"   âœ… æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        
        for article in articles:
            article['blog'] = name
            article['blog_desc'] = description
            all_articles.append(article)
        
        time.sleep(random.uniform(1, 2))
    
    # Sort by date (newest first)
    def extract_date(article):
        date_str = article.get('date', '')
        if not date_str:
            return ''
        # Simple date extraction - just use the string as-is for sorting
        return date_str
    
    all_articles.sort(key=extract_date, reverse=True)
    
    # Generate summary
    summary = f"""ğŸ“ RSS Feeds æ›´æ–°æŠ¥å‘Š ({time.strftime('%Y-%m-%d %H:%M')})

ğŸ“Š ç»Ÿè®¡:
â€¢ æˆåŠŸæŠ“å–: {len([a for a in all_articles if a.get('title')])} ç¯‡æ–‡ç« 
â€¢ æ¥æºåšå®¢: {len(blogs)} ä¸ª

ğŸ”¥ æœ€æ–°æ–‡ç« :

"""
    
    for i, article in enumerate(all_articles[:15], 1):
        summary += f"""{i}. **{article['title']}**
   ğŸ·ï¸ æ¥æº: {article['blog']} ({article['blog_desc']})
   ğŸ”— é“¾æ¥: {article['link']}
   ğŸ“… {article.get('date', 'æœªçŸ¥æ—¥æœŸ')}
   ğŸ“ {article['description']}

"""
    
    if len(all_articles) > 15:
        summary += f"\nğŸ“š è¿˜æœ‰ {len(all_articles) - 15} ç¯‡æ–‡ç« ...\n"
    
    summary += """
ğŸ’¡ æç¤º: è®¿é—® https://mrgolftech.github.io/rss-articles/ æŸ¥çœ‹å®Œæ•´å†…å®¹
"""
    
    print("\n" + "=" * 50)
    print("ğŸ“‹ æ‘˜è¦æŠ¥å‘Šå·²ç”Ÿæˆ")
    print("=" * 50)
    
    # Save summary
    with open('/root/openclaw/gw/docs/rss/latest_summary.md', 'w', encoding='utf-8') as f:
        f.write(summary)
    
    return summary

if __name__ == '__main__':
    summary = main()
    print(summary)